{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"name":"nb_robustness_tests.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bukochgPFg7s"},"source":["## Example - RobustnessTest\n","\n","This notebook shows the functionality of the RobustnessTest."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEhtGsfLp7dO","executionInfo":{"status":"ok","timestamp":1623659284827,"user_tz":-120,"elapsed":157742,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"725ffaa1-d3b3-4c0e-d988-6f3d3f2ec5c3"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MuA8v9jLp-pf","executionInfo":{"status":"ok","timestamp":1623665363264,"user_tz":-120,"elapsed":6281,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"20043c4e-b515-4c2b-90b6-cc9a7e51c610"},"source":["!pip install captum\n","!pip install opencv-python\n","\n","import torch\n","import torchvision\n","from torchvision import transforms\n","import numpy as np\n","import h5py\n","from tqdm import tqdm\n","from captum.attr import Saliency, IntegratedGradients\n","from pathlib import Path\n","import warnings\n","\n","# Retrieve source code.\n","from drive.MyDrive.Projects.xai_quantification_toolbox import * #import xaiquantificationtoolbox\n","\n","# Notebook settings.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: captum in /usr/local/lib/python3.7/dist-packages (0.3.1)\n","Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from captum) (1.8.1+cu101)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from captum) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from captum) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->captum) (3.7.4.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.4.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->captum) (1.15.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n","The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XqKzag4VFjHT"},"source":["### Load model, data and attributions."]},{"cell_type":"code","metadata":{"id":"V6Evswk_IIdE","executionInfo":{"status":"ok","timestamp":1623661049182,"user_tz":-120,"elapsed":18147,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}}},"source":["# Load pre-trained ResNet18 model.\n","model = torchvision.models.resnet18(pretrained=True)\n","model.eval()\n","\n","# Load test data and loaders.\n","test_set = torchvision.datasets.ImageFolder(root='/content/drive/My Drive/imagenet_images', \n","                                            transform=transforms.Compose([transforms.Resize(256),\n","                                                                          transforms.CenterCrop((224, 224)),\n","                                                                          transforms.ToTensor(),\n","                                                                          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n","test_loader = torch.utils.data.DataLoader(test_set, shuffle=True, batch_size=64)\n","\n","\n","# Evaluate model performance.\n","#predictions, labels = evaluate_model(model.to(device), data=test_loader, device=device)\n","#print(f\"\\nModel test accuracy: {(100 * score_model(predictions, labels)):.2f}%\")\n","\n","# Load data, targets and attributions.\n","x_batch, y_batch = iter(test_loader).next()\n","a_batch = explain(model.to(device), x_batch.to(device), y_batch.to(device), explanation_func=\"Saliency\")"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"3J-PX7f08ubz"},"source":["# Plot some explanations!\n","import matplotlib.pyplot as plt\n","\n","for i in range(20, 30): #[4140, 2091, 78, 1195]: \n","    plt.imshow(denormalize_image(x_batch.cpu().data[i]).transpose(0, 1).transpose(1, 2))\n","    plt.show()\n","    plt.imshow(a_batch.cpu().data[i], cmap=\"seismic\")\n","    plt.colorbar()\n","    plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPOL47-BFoQp"},"source":["### Option 1. Evaluate the robustness of attributions in one line of code."]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"3T0c9KtDFeqw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623661270511,"user_tz":-120,"elapsed":2203,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"4a0a0ff7-9eeb-4ce3-8bac-7e915c341851"},"source":["# One-liner to measure robustness of provided attributions.\n","scores = RobustnessTest(**{\n","    \"similarity_func\": lipschitz_constant,\n","    \"perturb_func\": gaussian_noise,\n","})(model=model, \n","   x_batch=x_batch.cpu().numpy(), \n","   y_batch=y_batch.cpu().numpy(), \n","   a_batch=a_batch.cpu().numpy(), \n","   device=device, \n","   **{\"explanation_func\": \"Saliency\"})\n","\n","scores"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[autoreload of drive.MyDrive.Projects.xai_quantification_toolbox.xai_quantification_toolbox.helpers.explanation_func failed: Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n","    superreload(m, reload, self.old_objects)\n","TypeError: Union[arg, ...]: each arg must be a type. Got <module 'torchvision.models' from '/usr/local/lib/python3.7/dist-packages/torchvision/models/__init_.\n","]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["[36.15371720125984,\n"," 21.459338637757334,\n"," 43.42919743262423,\n"," 37.08792479182698,\n"," 13.523527100876139,\n"," 20.94329984881238,\n"," 44.70699158950547,\n"," 23.67869291188523,\n"," 29.25100910964856,\n"," 24.647430136850215,\n"," 50.885522424134564,\n"," 27.256833951635276,\n"," 35.857669598139225,\n"," 23.510530839622227,\n"," 38.806234502506776,\n"," 22.85824393740396,\n"," 25.52348039434638,\n"," 37.083352979412076,\n"," 23.802950806915256,\n"," 35.60551562569577,\n"," 12.987899146346574,\n"," 41.11055848110009,\n"," 32.444884071396245,\n"," 23.838824690429323,\n"," 22.463970294103255,\n"," 38.40506339860404,\n"," 9.806916670998078,\n"," 20.191377891336156,\n"," 18.814474579961832,\n"," 21.044851109846004,\n"," 31.120535577480528,\n"," 20.508416900853167,\n"," 28.96616358275558,\n"," 28.041295090252827,\n"," 32.706932361397975,\n"," 28.4013933342942,\n"," 23.71318363130131,\n"," 30.811142791578604,\n"," 33.38204749130086,\n"," 31.117921891874843,\n"," 39.05896825072428,\n"," 27.431349474539743,\n"," 37.06734211738538,\n"," 26.39497906634696,\n"," 41.276337324742194,\n"," 24.21098467976126,\n"," 32.66730895680301,\n"," 40.45729727541629,\n"," 26.4409084470792,\n"," 13.361657896465234,\n"," 28.016743081793173,\n"," 20.530233238933572,\n"," 22.067828476799217,\n"," 44.07815887602082,\n"," 25.678726021812206,\n"," 25.36113910601404,\n"," 21.867089259288587,\n"," 37.1094314495018,\n"," 22.969985489511043,\n"," 29.91896959952532,\n"," 38.86501512142386,\n"," 48.53464767108493,\n"," 41.38675067318194,\n"," 25.541848947539048]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"YJ3a80khzOBV","executionInfo":{"status":"ok","timestamp":1623662076443,"user_tz":-120,"elapsed":247,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}}},"source":[""],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFsFfzrRJFph","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623662230357,"user_tz":-120,"elapsed":11415,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"f2fb6efc-b41a-4e26-bb67-a8518899798b"},"source":["\n","# One-liner to measure continuity of provided attributions.\n","scores = ContinuityTest(**{\n","    \"similarity_func\": correlation_spearman,\n","    \"perturb_func\": translation_x_direction,\n","    \"nr_patches\": 4,\n","    \"nr_steps\": 10,\n","})(model=model, \n","   x_batch=x_batch.cpu().numpy(), \n","   y_batch=y_batch.cpu().numpy(), \n","   a_batch=a_batch.cpu().numpy(), \n","   device=device, \n","   **{\"explanation_func\": \"Saliency\"})\n","\n","scores"],"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.4878787878787878,\n"," 0.6,\n"," 0.7999999999999999,\n"," 0.5878787878787878,\n"," 0.4818181818181818,\n"," 0.6242424242424243,\n"," 0.8272727272727272,\n"," 0.5303030303030303,\n"," 0.7727272727272727,\n"," 0.8545454545454544,\n"," 0.7030303030303029,\n"," 0.7818181818181817,\n"," 0.7303030303030302,\n"," 0.5969696969696969,\n"," 0.6909090909090908,\n"," 0.7424242424242424,\n"," 0.6303030303030303,\n"," 0.9151515151515152,\n"," 0.33030303030303027,\n"," 0.718181818181818,\n"," 0.5424242424242424,\n"," 0.39393939393939387,\n"," 0.7151515151515151,\n"," 0.3424242424242424,\n"," 0.903030303030303,\n"," 0.8454545454545455,\n"," 0.3848484848484847,\n"," 0.5393939393939393,\n"," 0.506060606060606,\n"," 0.2696969696969697,\n"," 0.4969696969696969,\n"," 0.796969696969697,\n"," 0.8303030303030303,\n"," 0.5909090909090909,\n"," 0.35454545454545455,\n"," 0.34545454545454546,\n"," 0.49999999999999994,\n"," 0.5393939393939393,\n"," 0.509090909090909,\n"," 0.5333333333333333,\n"," 0.7909090909090908,\n"," 0.8303030303030302,\n"," 0.7757575757575756,\n"," 0.718181818181818,\n"," 0.6757575757575757,\n"," 0.812121212121212,\n"," 0.7424242424242422,\n"," 0.5878787878787878,\n"," 0.7393939393939393,\n"," 0.5393939393939394,\n"," 0.48484848484848475,\n"," 0.6242424242424243,\n"," 0.4606060606060606,\n"," 0.6636363636363636,\n"," 0.5303030303030303,\n"," 0.7333333333333333,\n"," 0.5909090909090908,\n"," 0.9424242424242424,\n"," 0.5363636363636363,\n"," 0.8484848484848484,\n"," 0.6333333333333333,\n"," 0.615151515151515,\n"," 0.712121212121212,\n"," 0.6454545454545454]"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m1ybnpA8iWzI","executionInfo":{"status":"ok","timestamp":1623662290322,"user_tz":-120,"elapsed":1671,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"288bd9bf-dacf-43bc-e92d-5329becbadea"},"source":["# One-liner to measure input independence of provided attributions.\n","scores = InputIndependenceRate(**{\n","    \"similarity_func\": abs_difference,\n","    \"perturb_func\": optimization_scheme,\n","    \"std\": 0.01,\n","})(model=model, \n","   x_batch=x_batch.cpu().numpy(), \n","   y_batch=y_batch.cpu().numpy(), \n","   a_batch=a_batch.cpu().numpy(), \n","   device=device, \n","   **{\"explanation_func\": \"Saliency\"})\n","\n","scores"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQR-fgSXb7hD","executionInfo":{"status":"ok","timestamp":1623662400816,"user_tz":-120,"elapsed":110497,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"3e248b78-fcd1-4abe-e4a2-3a2baacee7ff"},"source":["# One-liner to measure local lipschitz constant of provided attributions.\n","scores = EstimatedLocalLipschitzConstant(**{\n","    \"similarity_func\": lipschitz_constant,\n","    \"perturb_func\": gaussian_noise,\n","    \"distance_numerator\": distance_euclidean,\n","    \"distance_denominator\": distance_euclidean,\n","    \"perturb_std\": 0.1,\n","    \"nr_steps\": 10,\n","})(model=model, \n","   x_batch=x_batch.cpu().numpy(), \n","   y_batch=y_batch.cpu().numpy(), \n","   a_batch=a_batch.cpu().numpy(), \n","   device=device, \n","   **{\"explanation_func\": \"Saliency\"})\n","\n","scores"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[37.30127575118598,\n"," 22.121814107683537,\n"," 45.067826177396114,\n"," 39.232404747828,\n"," 14.437890142892192,\n"," 22.202293938019196,\n"," 45.9585545621177,\n"," 24.456367302295472,\n"," 30.57050645608584,\n"," 26.44729388104142,\n"," 54.052140853975125,\n"," 28.8199717935829,\n"," 37.18822060425881,\n"," 24.189395625404025,\n"," 40.529155887407825,\n"," 24.487054122209425,\n"," 26.603376709913938,\n"," 37.90717359248646,\n"," 25.241728448144286,\n"," 36.94950261647965,\n"," 14.21280285975028,\n"," 41.24405426106984,\n"," 34.78548224497834,\n"," 24.558473536604215,\n"," 22.712949325246996,\n"," 39.56552581615689,\n"," 10.481931323168332,\n"," 21.126266072565624,\n"," 19.233047753960527,\n"," 22.323757902618798,\n"," 33.82471535585488,\n"," 22.653545378826532,\n"," 31.47558044439554,\n"," 28.46441162660213,\n"," 36.54705731638505,\n"," 30.017435091098957,\n"," 24.35241438395752,\n"," 33.228235072373764,\n"," 34.68023558398905,\n"," 32.4090633205909,\n"," 42.98214852031414,\n"," 30.940671220760954,\n"," 38.26997863420922,\n"," 27.307556240335924,\n"," 42.498760145207825,\n"," 25.67364961895835,\n"," 35.4298964453991,\n"," 43.074363599307304,\n"," 29.72153336288962,\n"," 14.798905012274746,\n"," 29.097419573647308,\n"," 21.78389489550761,\n"," 23.37526793215149,\n"," 46.39869499207965,\n"," 26.59403263944377,\n"," 26.679024220106747,\n"," 23.570887249542015,\n"," 40.38783254250594,\n"," 24.59377511668515,\n"," 34.05953418487948,\n"," 41.02435526902164,\n"," 52.728352198563876,\n"," 49.12473353710254,\n"," 27.00307747731729]"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bwftK8AB7cQM","executionInfo":{"status":"ok","timestamp":1623666535606,"user_tz":-120,"elapsed":8564,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"581853c1-c912-4443-c799-8d9c8e853d3d"},"source":["# One-liner to measure local lipschitz constant of provided attributions.\n","scores = SensitivityMax(**{\n","    \"similarity_func\": difference,\n","    \"perturb_func\": uniform_sampling,\n","    \"norm_numerator\": fro_norm,\n","    \"norm_denominator\": fro_norm,\n","    \"perturb_radius\": 0.02,\n","    \"nr_steps\": 10,\n","})(model=model, \n","   x_batch=x_batch.cpu().numpy(), \n","   y_batch=y_batch.cpu().numpy(), \n","   a_batch=a_batch.cpu().numpy(), \n","   device=device, \n","   **{\"explanation_func\": \"Saliency\"})\n","\n","scores"],"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.0137402145,\n"," 0.0064482414,\n"," 0.014117877,\n"," 0.016248235,\n"," 0.0034813983,\n"," 0.0076988307,\n"," 0.015668634,\n"," 0.008358887,\n"," 0.007957364,\n"," 0.0075780484,\n"," 0.015087734,\n"," 0.008030167,\n"," 0.01583539,\n"," 0.009883713,\n"," 0.020468712,\n"," 0.009848966,\n"," 0.006580384,\n"," 0.016781868,\n"," 0.010596507,\n"," 0.01325135,\n"," 0.003844463,\n"," 0.015161874,\n"," 0.011751871,\n"," 0.005740982,\n"," 0.0074924245,\n"," 0.017333066,\n"," 0.002706792,\n"," 0.007640656,\n"," 0.006835906,\n"," 0.010460794,\n"," 0.009770189,\n"," 0.012336688,\n"," 0.011851529,\n"," 0.009166229,\n"," 0.011856076,\n"," 0.011575303,\n"," 0.011354601,\n"," 0.010320113,\n"," 0.011020264,\n"," 0.009523336,\n"," 0.014258611,\n"," 0.009800352,\n"," 0.015084063,\n"," 0.010181025,\n"," 0.013268139,\n"," 0.009647754,\n"," 0.022006867,\n"," 0.01575007,\n"," 0.010010494,\n"," 0.004446627,\n"," 0.009163492,\n"," 0.011745132,\n"," 0.009597097,\n"," 0.012944428,\n"," 0.007821564,\n"," 0.009619963,\n"," 0.0071163746,\n"," 0.01375225,\n"," 0.010869938,\n"," 0.012274247,\n"," 0.014073269,\n"," 0.02050844,\n"," 0.019222727,\n"," 0.0071925684]"]},"metadata":{"tags":[]},"execution_count":67}]},{"cell_type":"code","metadata":{"id":"0efQst0pb7th"},"source":["\n","from copy import deepcopy\n","from inspect import signature\n","from typing import Any, Callable, Tuple, Union, cast\n","\n","import torch\n","from torch import Tensor\n","\n","from captum._utils.common import (\n","    _expand_and_update_additional_forward_args,\n","    _expand_and_update_baselines,\n","    _expand_and_update_target,\n","    _format_baseline,\n","    _format_input,\n","    _format_tensor_into_tuples,\n",")\n","from captum._utils.typing import TensorOrTupleOfTensorsGeneric\n","from captum.log import log_usage\n","from captum.metrics._utils.batching import _divide_and_aggregate_metrics\n","\n","\n","#[docs] @ log_usage()\n","\n","def sensitivity_max(\n","        explanation_func: Callable,\n","        inputs: TensorOrTupleOfTensorsGeneric,\n","        perturb_func: Callable = default_perturb_func,\n","        perturb_radius: float = 0.02,\n","        n_perturb_samples: int = 10,\n","        norm_ord: str = \"fro\",\n","        max_examples_per_batch: int = None,\n","        **kwargs: Any,\n",") -> Tensor:\n","    r\"\"\"\n","    Explanation sensitivity measures the extent of explanation change when\n","    the input is slightly perturbed. It has been shown that the models that\n","    have high explanation sensitivity are prone to adversarial attacks:\n","    `Interpretation of Neural Networks is Fragile`\n","    https://www.aaai.org/ojs/index.php/AAAI/article/view/4252\n","\n","    `sensitivity_max` metric measures maximum sensitivity of an explanation\n","    using Monte Carlo sampling-based approximation. By default in order to\n","    do so it samples multiple data points from a sub-space of an L-Infinity\n","    ball that has a `perturb_radius` radius using `default_perturb_func`\n","    default perturbation function. In a general case users can\n","    use any L_p ball or any other custom sampling technique that they\n","    prefer by providing a custom `perturb_func`.\n","\n","    Note that max sensitivity is similar to Lipschitz Continuity metric\n","    however it is more robust and easier to estimate.\n","    Since the explanation, for instance an attribution function,\n","    may not always be continuous, can lead to unbounded\n","    Lipschitz continuity. Therefore the latter isn't always appropriate.\n","\n","    More about the Lipschitz Continuity Metric can also be found here\n","    `On the Robustness of Interpretability Methods`\n","    https://arxiv.org/pdf/1806.08049.pdf\n","    and\n","    `Towards Robust Interpretability with Self-Explaining Neural Networks`\n","    https://papers.nips.cc/paper\\\n","    8003-towards-robust-interpretability-\n","    with-self-explaining-neural-networks.pdf\n","\n","    More details about sensitivity max can be found here:\n","    `On the (In)fidelity and Sensitivity of Explanations`\n","    https://arxiv.org/pdf/1901.09392.pdf\n","\n","    Args:\n","\n","        explanation_func (callable):\n","                This function can be the `attribute` method of an\n","                attribution algorithm or any other explanation method\n","                that returns the explanations.\n","\n","        inputs (tensor or tuple of tensors):  Input for which\n","                explanations are computed. If `explanation_func` takes a\n","                single tensor as input, a single input tensor should\n","                be provided.\n","                If `explanation_func` takes multiple tensors as input, a tuple\n","                of the input tensors should be provided. It is assumed\n","                that for all given input tensors, dimension 0 corresponds\n","                to the number of examples (aka batch size), and if\n","                multiple input tensors are provided, the examples must\n","                be aligned appropriately.\n","\n","        perturb_func (callable):\n","                The perturbation function of model inputs. This function takes\n","                model inputs and optionally `perturb_radius` if\n","                the function takes more than one argument and returns\n","                perturbed inputs.\n","\n","                If there are more than one inputs passed to sensitivity function those\n","                will be passed to `perturb_func` as tuples in the same order as they\n","                are passed to sensitivity function.\n","\n","                It is important to note that for performance reasons `perturb_func`\n","                isn't called for each example individually but on a batch of\n","                input examples that are repeated `max_examples_per_batch / batch_size`\n","                times within the batch.\n","\n","            Default: default_perturb_func\n","        perturb_radius (float, optional): The epsilon radius used for sampling.\n","            In the `default_perturb_func` it is used as the radius of\n","            the L-Infinity ball. In a general case it can serve as a radius of\n","            any L_p nom.\n","            This argument is passed to `perturb_func` if it takes more than\n","            one argument.\n","\n","            Default: 0.02\n","        n_perturb_samples (int, optional): The number of times input tensors\n","                are perturbed. Each input example in the inputs tensor is\n","                expanded `n_perturb_samples` times before calling\n","                `perturb_func` function.\n","\n","                Default: 10\n","        norm_ord (int, float, inf, -inf, 'fro', 'nuc', optional): The type of norm\n","                that is used to compute the\n","                norm of the sensitivity matrix which is defined as the difference\n","                between the explanation function at its input and perturbed input.\n","\n","                Default: 'fro'\n","        max_examples_per_batch (int, optional): The number of maximum input\n","                examples that are processed together. In case the number of\n","                examples (`input batch size * n_perturb_samples`) exceeds\n","                `max_examples_per_batch`, they will be sliced\n","                into batches of `max_examples_per_batch` examples and processed\n","                in a sequential order. If `max_examples_per_batch` is None, all\n","                examples are processed together. `max_examples_per_batch` should\n","                at least be equal `input batch size` and at most\n","                `input batch size * n_perturb_samples`.\n","\n","                Default: None\n","         **kwargs (Any, optional): Contains a list of arguments that are passed\n","                to `explanation_func` explanation function which in some cases\n","                could be the `attribute` function of an attribution algorithm.\n","                Any additional arguments that need be passed to the explanation\n","                function should be included here.\n","                For instance, such arguments include:\n","                `additional_forward_args`, `baselines` and `target`.\n","\n","    Returns:\n","\n","        sensitivities (tensor): A tensor of scalar sensitivity scores per\n","               input example. The first dimension is equal to the\n","               number of examples in the input batch and the second\n","               dimension is one. Returned sensitivities are normalized by\n","               the magnitudes of the input explanations.\n","\n","    Examples::\n","        >>> # ImageClassifier takes a single input tensor of images Nx3x32x32,\n","        >>> # and returns an Nx10 tensor of class probabilities.\n","        >>> net = ImageClassifier()\n","        >>> saliency = Saliency(net)\n","        >>> input = torch.randn(2, 3, 32, 32, requires_grad=True)\n","        >>> # Computes sensitivity score for saliency maps of class 3\n","        >>> sens = sensitivity_max(saliency.attribute, input, target = 3)\n","\n","    \"\"\"\n","\n","    def _generate_perturbations(\n","            current_n_perturb_samples: int,\n","    ) -> TensorOrTupleOfTensorsGeneric:\n","        r\"\"\"\n","        The perturbations are generated for each example\n","        `current_n_perturb_samples` times.\n","\n","        For perfomance reasons we are not calling `perturb_func` on each example but\n","        on a batch that contains `current_n_perturb_samples` repeated instances\n","        per example.\n","        \"\"\"\n","        inputs_expanded: Union[Tensor, Tuple[Tensor, ...]] = tuple(\n","            torch.repeat_interleave(input, current_n_perturb_samples, dim=0)\n","            for input in inputs\n","        )\n","        if len(inputs_expanded) == 1:\n","            inputs_expanded = inputs_expanded[0]\n","\n","        return (\n","            perturb_func(inputs_expanded, perturb_radius)\n","            if len(signature(perturb_func).parameters) > 1\n","            else perturb_func(inputs_expanded)\n","        )\n","\n","    def max_values(input_tnsr: Tensor) -> Tensor:\n","        return torch.max(input_tnsr, dim=1).values  # type: ignore\n","\n","    kwarg_expanded_for = None\n","    kwargs_copy: Any = None\n","\n","    def _next_sensitivity_max(current_n_perturb_samples: int) -> Tensor:\n","        inputs_perturbed = _generate_perturbations(current_n_perturb_samples)\n","\n","        # copy kwargs and update some of the arguments that need to be expanded\n","        nonlocal kwarg_expanded_for\n","        nonlocal kwargs_copy\n","        if (\n","                kwarg_expanded_for is None\n","                or kwarg_expanded_for != current_n_perturb_samples\n","        ):\n","            kwarg_expanded_for = current_n_perturb_samples\n","            kwargs_copy = deepcopy(kwargs)\n","            _expand_and_update_additional_forward_args(\n","                current_n_perturb_samples, kwargs_copy\n","            )\n","            _expand_and_update_target(current_n_perturb_samples, kwargs_copy)\n","            if \"baselines\" in kwargs:\n","                baselines = kwargs[\"baselines\"]\n","                baselines = _format_baseline(\n","                    baselines, cast(Tuple[Tensor, ...], inputs)\n","                )\n","                if (\n","                        isinstance(baselines[0], Tensor)\n","                        and baselines[0].shape == inputs[0].shape\n","                ):\n","                    _expand_and_update_baselines(\n","                        cast(Tuple[Tensor, ...], inputs),\n","                        current_n_perturb_samples,\n","                        kwargs_copy,\n","                    )\n","\n","        expl_perturbed_inputs = explanation_func(inputs_perturbed, **kwargs_copy)\n","\n","        # tuplize `expl_perturbed_inputs` in case it is not\n","        expl_perturbed_inputs = _format_tensor_into_tuples(expl_perturbed_inputs)\n","\n","        expl_inputs_expanded = tuple(\n","            expl_input.repeat_interleave(current_n_perturb_samples, dim=0)\n","            for expl_input in expl_inputs\n","        )\n","\n","        sensitivities = torch.cat(\n","            [\n","                (expl_input - expl_perturbed).view(expl_perturbed.size(0), -1)\n","                for expl_perturbed, expl_input in zip(\n","                expl_perturbed_inputs, expl_inputs_expanded\n","            )\n","            ],\n","            dim=1,\n","        )\n","        # compute the norm of original input explanations\n","        expl_inputs_norm_expanded = torch.norm(\n","            torch.cat(\n","                [expl_input.view(expl_input.size(0), -1) for expl_input in expl_inputs],\n","                dim=1,\n","            ),\n","            p=norm_ord,\n","            dim=1,\n","            keepdim=True,\n","        ).repeat_interleave(current_n_perturb_samples, dim=0)\n","        expl_inputs_norm_expanded = torch.where(\n","            expl_inputs_norm_expanded == 0.0,\n","            torch.tensor(\n","                1.0,\n","                device=expl_inputs_norm_expanded.device,\n","                dtype=expl_inputs_norm_expanded.dtype,\n","            ),\n","            expl_inputs_norm_expanded,\n","        )\n","\n","        # compute the norm for each input noisy example\n","        sensitivities_norm = (\n","                torch.norm(sensitivities, p=norm_ord, dim=1, keepdim=True)\n","                / expl_inputs_norm_expanded\n","        )\n","        return max_values(sensitivities_norm.view(bsz, -1))\n","\n","    inputs = _format_input(inputs)  # type: ignore\n","\n","    bsz = inputs[0].size(0)\n","\n","    with torch.no_grad():\n","        expl_inputs = explanation_func(inputs, **kwargs)\n","        metrics_max = _divide_and_aggregate_metrics(\n","            cast(Tuple[Tensor, ...], inputs),\n","            n_perturb_samples,\n","            _next_sensitivity_max,\n","            max_examples_per_batch=max_examples_per_batch,\n","            agg_func=torch.max,\n","        )\n","    return metrics_max"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g-15GcNHFlGK"},"source":["### Option 2. Evaluate the robustness of provided attributions while enjoying more functionality of Quantifier and Plotting."]},{"cell_type":"code","metadata":{"id":"2C-TtWUNLCEP"},"source":["# Provide notebooks for the different use cases: compare models, XAI methods, different measures\n","# ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"32DTrDGiFeqw"},"source":["# Specify the tests.\n","tests = [RobustnessTest(**{\n","    \"similarity_function\": similarity_fn,\n","    \"perturbation_function\": gaussian_blur,\n","}) for similarity_fn in [lipschitz_constant, distance_euclidean, cosine]]\n","\n","# Load attributions of another explanation method.\n","a_batch_intgrad = IntegratedGradients(model).attribute(inputs=x_batch, targets=y_batch)\n","\n","# Init the quantifier object.\n","quantifier = Quantifier(measures=tests, io_object=h5py.File(\"PATH_TO_H5PY_FILE\"), checkpoints=..)\n","\n","# Score the tests.\n","results = [quantifier.score(model=model, x_batch=x_batch, y_batch=y_batch, a_batch=a_batch)\n","           for a_batch in [a_batch_saliency, a_batch_intgrad]]\n","\n","# Plot Saliency vs Integrated Gradients.\n","Plotting(results, show=False, path_to_save=\"PATH_TO_SAVE_FIGURE\")"],"execution_count":null,"outputs":[]}]}