{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"name":"nb_test_measures.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bukochgPFg7s"},"source":["## Example - RobustnessTest\n","\n","This notebook shows the functionality of the RobustnessTest."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEhtGsfLp7dO","executionInfo":{"status":"ok","timestamp":1623787231610,"user_tz":-120,"elapsed":646,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"310aca51-1abc-43ad-95cf-4a0118de0a1c"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MuA8v9jLp-pf","executionInfo":{"status":"ok","timestamp":1623787174028,"user_tz":-120,"elapsed":4813,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"33f7af3e-faec-4745-a66b-950a6e175d0a"},"source":["!pip install captum\n","!pip install opencv-python\n","\n","import torch\n","import torchvision\n","from torchvision import transforms\n","import numpy as np\n","import h5py\n","from tqdm import tqdm\n","from captum.attr import Saliency, IntegratedGradients\n","from pathlib import Path\n","import warnings\n","\n","# Retrieve source code.\n","from drive.MyDrive.Projects.xai_quantification_toolbox import * #import xaiquantificationtoolbox\n","\n","# Notebook settings.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: captum in /usr/local/lib/python3.7/dist-packages (0.3.1)\n","Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from captum) (1.8.1+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from captum) (1.19.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from captum) (3.2.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->captum) (3.7.4.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (1.3.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.8.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->captum) (1.15.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n","The autoreload extension is already loaded. To reload it, use:\n","  %reload_ext autoreload\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8bzl32YHhsGf"},"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XqKzag4VFjHT"},"source":["### Load model, data and attributions."]},{"cell_type":"code","metadata":{"id":"V6Evswk_IIdE"},"source":["# Load pre-trained ResNet18 model.\n","model = torchvision.models.resnet18(pretrained=True)\n","model.eval()\n","\n","# Load test data and loaders.\n","test_set = torchvision.datasets.ImageFolder(root='/content/drive/My Drive/imagenet_images', \n","                                            transform=transforms.Compose([transforms.Resize(256),\n","                                                                          transforms.CenterCrop((224, 224)),\n","                                                                          transforms.ToTensor(),\n","                                                                          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n","test_loader = torch.utils.data.DataLoader(test_set, shuffle=True, batch_size=8)\n","\n","\n","# Evaluate model performance.\n","#predictions, labels = evaluate_model(model.to(device), data=test_loader, device=device)\n","#print(f\"\\nModel test accuracy: {(100 * score_model(predictions, labels)):.2f}%\")\n","\n","# Load data, targets and attributions.\n","x_batch, y_batch = iter(test_loader).next()\n","a_batch = explain(model.to(device), x_batch.to(device), y_batch.to(device), explanation_func=\"Saliency\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3J-PX7f08ubz","executionInfo":{"status":"ok","timestamp":1623787518468,"user_tz":-120,"elapsed":4,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}}},"source":["\"\"\"\n","# Plot some explanations!\n","import matplotlib.pyplot as plt\n","\n","for i in range(20, 30): #[4140, 2091, 78, 1195]: \n","    plt.imshow(denormalize_image(x_batch.cpu().data[i]).transpose(0, 1).transpose(1, 2))\n","    plt.show()\n","    plt.imshow(a_batch.cpu().data[i], cmap=\"seismic\")\n","    plt.colorbar()\n","    plt.show()\n","\"\"\";"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pGaOiwWd4ZFz"},"source":["### Robustness tests"]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"3T0c9KtDFeqw"},"source":["# One-liner to measure robustness of provided attributions.\n","scores = RobustnessTest(**{\n","    \"similarity_func\": lipschitz_constant,\n","    \"perturb_func\": gaussian_noise,\n","})(model=model, \n","   x_batch=x_batch.cpu().numpy(), \n","   y_batch=y_batch.cpu().numpy(), \n","   a_batch=a_batch.cpu().numpy(), \n","   **{\"explanation_func\": \"Saliency\", \"device\": device})\n","\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yFsFfzrRJFph"},"source":["# One-liner to measure continuity of provided attributions.\n","scores = ContinuityTest(**{\n","    \"similarity_func\": correlation_spearman,\n","    \"perturb_func\": translation_x_direction,\n","    \"nr_patches\": 4,\n","    \"nr_steps\": 10,\n","})(model=model, \n","   x_batch=x_batch.cpu().numpy(),\n","   y_batch=y_batch.cpu().numpy(),\n","   a_batch=a_batch.cpu().numpy(),\n","   **{\"explanation_func\": \"Saliency\", \"device\": device})\n","\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1ybnpA8iWzI","executionInfo":{"status":"aborted","timestamp":1623787521087,"user_tz":-120,"elapsed":7,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}}},"source":["# One-liner to measure input independence of provided attributions.\n","scores = InputIndependenceRate(**{\n","    \"similarity_func\": abs_difference,\n","    \"perturb_func\": optimization_scheme, # TODO.\n","    \"perturb_std\": 0.01,\n","    \"threshold\": 0.1,\n","})(model=model, \n","   x_batch=x_batch.cpu().numpy(), \n","   y_batch=y_batch.cpu().numpy(), \n","   a_batch=a_batch.cpu().numpy(), \n","   **{\"explanation_func\": \"Saliency\", \"device\": device})\n","\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQR-fgSXb7hD","executionInfo":{"status":"aborted","timestamp":1623787521088,"user_tz":-120,"elapsed":8,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}}},"source":["# One-liner to measure local lipschitz constant of provided attributions.\n","scores = LocalLipschitzEstimate(**{\n","    \"similarity_func\": lipschitz_constant,\n","    \"perturb_func\": gaussian_noise,\n","    \"distance_numerator\": distance_euclidean,\n","    \"distance_denominator\": distance_euclidean,\n","    \"perturb_std\": 0.1,\n","    \"nr_steps\": 10, #200\n","})(model=model, \n","   x_batch=x_batch.cpu().numpy(), \n","   y_batch=y_batch.cpu().numpy(), \n","   a_batch=a_batch.cpu().numpy(), \n","   **{\"explanation_func\": \"Saliency\", \"device\": device})\n","\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bwftK8AB7cQM","executionInfo":{"status":"aborted","timestamp":1623787521089,"user_tz":-120,"elapsed":9,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}}},"source":["# One-liner to measure local lipschitz constant of provided attributions.\n","scores = SensitivityMax(**{\n","    \"similarity_func\": difference,\n","    \"perturb_func\": uniform_sampling,\n","    \"norm_numerator\": fro_norm,\n","    \"norm_denominator\": fro_norm,\n","    \"perturb_radius\": 0.2,\n","    \"nr_steps\": 10,\n","})(model=model, \n","   x_batch=x_batch.cpu().numpy(), \n","   y_batch=y_batch.cpu().numpy(), \n","   a_batch=a_batch.cpu().numpy(), \n","   **{\"explanation_func\": \"Saliency\", \"device\": device})\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVvjK-Zt4j3j"},"source":["### Faithfulness tests"]},{"cell_type":"code","metadata":{"id":"vqNSdIRndI3i"},"source":["# One-liner to for faithfulness base class of provided attributions.\n","scores = FaithfulnessTest(**{\n","    \"perturb_func\": baseline_replacement_by_indices,\n","    \"similarity_func\": correlation_spearman,\n","    \"perturb_baseline\": 0.0,  \n","    \"pixels_in_step\": 128,\n","})(model=model, \n","   x_batch=x_batch.cpu().numpy(), \n","   y_batch=y_batch.cpu().numpy(), \n","   a_batch=a_batch.cpu().numpy(), \n","   **{\"explanation_func\": \"Saliency\", \"device\": device})\n","\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"asdkkZrQ4ksH"},"source":["# One-liner to measure faithfulness estimate of provided attributions.\n","scores = FaithfulnessEstimate(**{\n","    \"perturb_func\": replacement_by_indices,\n","    \"similarity_func\": correlation_pearson,\n","    \"perturb_baseline\": 0.0,  \n","    \"pixels_in_step\": 8,\n","})(model=model, \n","   x_batch=x_batch.cpu().numpy(),\n","   y_batch=y_batch.cpu().numpy(), \n","   a_batch=a_batch.cpu().numpy(), \n","   **{\"explanation_func\": \"Saliency\", \"device\": device})\n","\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X7F4gzqS7XWW"},"source":["# One-liner to measure infidelity of provided attributions.\n","scores = Infidelity(**{\n","    \"perturb_func\": baseline_replacement_by_patch,\n","    \"similarity_func\": mse,\n","    \"perturb_baseline\": \"black\",  \n","    \"perturb_patch_sizes\": [14, 28] #list(np.arange(10,30)),\n","})(model=model, \n","  x_batch=x_batch.cpu().numpy(), \n","  y_batch=y_batch.cpu().numpy(), \n","  a_batch=a_batch.cpu().numpy(), \n","  **{\"explanation_func\": \"Saliency\", \"device\": device})\n","\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uNTzwhfgLvrI"},"source":["\n","class MonotonicityMetric(FaithfulnessTest):\n","    \"\"\"\n","    Implementation of Montonicity Metric by Luss at el., 2019.\n","\n","    It captures attributions' faithfulness by incrementally adding each attribute\n","    in order of increasing importance and evaluating the effect on model performance.\n","    As more features are added, the performance of the model is expected to increase\n","    and thus result in monotonically increasing model performance.\n","\n","    References:\n","        Luss, Ronny, et al. \"Generating contrastive explanations with monotonic attribute functions.\" \n","        arXiv preprint arXiv:1905.12698 (2019).\n","\n","    Current assumptions:\n","        • ...\n","    \"\"\"\n","\n","    def __init__(self, *args, **kwargs):\n","        self.args = args\n","        self.kwargs = kwargs\n","        self.similarity_func = self.kwargs.get(\"similarity_func\", correlation_pearson)\n","        self.perturb_func = self.kwargs.get(\"perturb_func\", baseline_replacement_by_indices)\n","        self.perturb_baseline = self.kwargs.get(\"perturb_baseline\", 0.0)\n","\n","        self.img_size = self.kwargs.get(\"img_size\", 224)\n","        self.nr_channels = self.kwargs.get(\"nr_channels\", 3)\n","\n","        self.pixels_in_step = self.kwargs.get(\"pixels_in_step\", 1)\n","        assert (\n","                           self.img_size * self.img_size) % self.pixels_in_step == 0, \"Set 'pixels_in_step' so that the modulo remainder returns 0 given the image size.\"\n","        self.max_steps_per_input = self.kwargs.get(\"max_steps_per_input\", None)\n","\n","        if self.max_steps_per_input is not None:\n","            assert (\n","                               self.img_size * self.img_size) % self.max_steps_per_input == 0, \"Set 'max_steps_per_input' so that the modulo remainder returns 0 given the image size.\"\n","            self.pixels_in_step = (self.img_size * self.img_size) / self.max_steps_per_input\n","\n","        super(FaithfulnessTest, self).__init__()\n","\n","    def __call__(\n","            self,\n","            model,\n","            x_batch: np.array,\n","            y_batch: Union[np.array, int],\n","            a_batch: Union[np.array, None],\n","            **kwargs\n","    ):\n","        assert (\n","                \"explanation_func\" in kwargs\n","        ), \"To run RobustnessTest specify 'explanation_func' (str) e.g., 'Gradient'.\"\n","        assert (\n","                np.shape(x_batch)[0] == np.shape(a_batch)[0]\n","        ), \"Inputs and attributions should include the same number of samples.\"\n","\n","        if a_batch is None:\n","            explain(\n","                model.to(kwargs.get(\"device\", None)),\n","                x_batch,\n","                y_batch,\n","                explanation_func=kwargs.get(\"explanation_func\", \"Gradient\"),\n","                device=kwargs.get(\"device\", None),\n","            )\n","\n","        results = []\n","\n","        for ix, (x, y, a) in enumerate(zip(x_batch, y_batch, a_batch)):\n","\n","            # Get indices of sorted attributions (descending).\n","            a = abs(a.flatten())\n","            a_indices = np.argsort(a)\n","\n","            # Predict on input.\n","            with torch.no_grad():\n","                y_pred = float(model(\n","                    torch.Tensor(x)\n","                        .reshape(1, self.nr_channels, self.img_size, self.img_size)\n","                        .to(kwargs.get(\"device\", None)))[:, y])\n","\n","            preds = np.zeros(self.img_size*self.img_size)\n","\n","            for i_ix, a_ix in enumerate(a_indices[::self.pixels_in_step]):\n","\n","                if i_ix == 0:\n","                    a_ix = a_indices[:self.pixels_in_step]\n","                else:\n","                    a_ix = a_indices[(self.pixels_in_step * i_ix):(self.pixels_in_step * (i_ix + 1))]\n","\n","                x_perturbed = self.perturb_func(img=x.flatten(),\n","                                                **{\"index\": a_ix, \"perturb_baseline\": self.perturb_baseline})\n","                # Predict on perturbed input x.\n","                with torch.no_grad():\n","                    y_pred_i = float(model(\n","                        torch.Tensor(x_perturbed)\n","                            .reshape(1, self.nr_channels, self.img_size, self.img_size)\n","                            .to(kwargs.get(\"device\", None)))[:, y])\n","                preds[i_ix] = float(y_pred_i)\n","            \n","            results.append(self.similarity_func(a=att_sum, b=pred_deltas))\n","            #np.all(np.diff(y_pred_i[a_indices]) >= 0)\n","            \n","\n","        return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g_6bztd0NOc9"},"source":["# One-liner to measure faithfulness estimate of provided attributions.\n","scores = MonotonicityMetric(**{\n","    \"perturb_func\": replacement_by_indices,\n","    \"similarity_func\": correlation_pearson,\n","    \"perturb_baseline\": 0.0,  \n","    \"pixels_in_step\": 8,\n","})(model=model, \n","   x_batch=x_batch.cpu().numpy(),\n","   y_batch=y_batch.cpu().numpy(), \n","   a_batch=a_batch.cpu().numpy(), \n","   **{\"explanation_func\": \"Saliency\", \"device\": device})\n","\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPOL47-BFoQp"},"source":["### Option 1. Evaluate the robustness of attributions in one line of code."]},{"cell_type":"markdown","metadata":{"id":"g-15GcNHFlGK"},"source":["### Option 2. Evaluate the robustness of provided attributions while enjoying more functionality of Quantifier and Plotting."]},{"cell_type":"code","metadata":{"id":"2C-TtWUNLCEP"},"source":["# Provide notebooks for the different use cases: compare models, XAI methods, different measures\n","# ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"32DTrDGiFeqw"},"source":["# Specify the tests.\n","tests = [RobustnessTest(**{\n","    \"similarity_function\": similarity_fn,\n","    \"perturbation_function\": gaussian_blur,\n","}) for similarity_fn in [lipschitz_constant, distance_euclidean, cosine]]\n","\n","# Load attributions of another explanation method.\n","a_batch_intgrad = IntegratedGradients(model).attribute(inputs=x_batch, targets=y_batch)\n","\n","# Init the quantifier object.\n","quantifier = Quantifier(measures=tests, io_object=h5py.File(\"PATH_TO_H5PY_FILE\"), checkpoints=..)\n","\n","# Score the tests.\n","results = [quantifier.score(model=model, x_batch=x_batch, y_batch=y_batch, a_batch=a_batch)\n","           for a_batch in [a_batch_saliency, a_batch_intgrad]]\n","\n","# Plot Saliency vs Integrated Gradients.\n","Plotting(results, show=False, path_to_save=\"PATH_TO_SAVE_FIGURE\")"],"execution_count":null,"outputs":[]}]}