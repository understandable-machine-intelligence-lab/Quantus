class RelativeRepresentationStability(Metric):
    """
    Relative Output Stability leverages the stability of an explanation with respect
    to the change in the output logits

    :math:`RRS(x, x', ex, ex') = max \\frac{||\\frac{e_x - e_{x'}}{e_x}||_p}{max (||\\frac{L_x - L_{x'}}{L_x}||_p, \epsilon_{min})},`
    where `L(Â·)` denotes the internal model representation, e.g., output embeddings of hidden layers.

    References:
        1) Chirag Agarwal, et. al., 2022. "Rethinking stability for attribution based explanations." https://arxiv.org/pdf/2203.06877.pdf
    """

    def __init__(self, *args, **kwargs):
        """
        Parameters:
            args: not used
        Keyword arguments:
            abs: a bool stating if absolute operation should be taken on the attributions
            normalise: a bool stating if the attributions should be normalised
            normalise_func: a Callable that make a normalising transformation of the attributions
            display_progressbar (boolean): indicates whether a tqdm-progress-bar is printed, default=False.
            return_aggregate: a bool if an aggregated score should be produced for the metric over all instances
            aggregate_func: a Callable to aggregate the scores per instance to one float
            eps_min: Optional[float], a small constant to prevent division by 0 in relative_stability_objective, default 1e-6
            num_perturbations: Optional[int] number of times perturb_func should be executed, default 50
        """

        super().__init__(*args, **kwargs)
        self.num_perturbations = kwargs.get("num_perturbations", 50)
        self.eps_min = kwargs.get("eps_min", 1e-6)

    def __call__(
        self,
        model,
        x_batch: np.ndarray,
        y_batch: Optional[np.ndarray],
        a_batch: Optional[np.ndarray] = None,
        *args,
        **kwargs,
    ) -> np.ndarray | float:
        """
        Parameters:
            model: instance of tf.keras.Model or torch.nn.Module
            x_batch: np.ndarray, a 4D tensor representing batch of input images
            y_batch: Optional[np.ndarray], a 1D tensor, representing labels for x_batch. Can be none, if `xs_batch`, `a_batch` and `as_batch` were provided.
            a_batch: Optional[np.ndarray], a 4D tensor with pre-computed explanations for the x_batch
        Keyword Arguments:
            perturb_func: Optional[Callable], a function used to perturbate inputs, must be provided unless no xs_batch provided
            xs_batch: Optional[np.ndarray], a 5D tensor representing perturbations of the x_batch, which results in the same labels (optional)
            explain_func: Optional[Callable], a function used to generate explanations, must be provided unless a_batch, as_batch were not provided
            as_batch: Optional[np.ndarray], a 5D tensor with pre-computed explanations for the xs_batch
            device: Optional[str|torch.device], a device on which torch should perform computations
            layer_names: Optional[List[str]], names of the layers internal representations of which should be used for objective computation
            layer_indices: Optional[List[int]], indices of the layers internal representation of which should be used for objective computation
            kwargs: additional kwargs, which are passed to perturb_func, explain_func.
        Returns:
            ris: float in case `return_aggregate=True`, otherwise np.ndarray of floats

        For each image `x`:
         - generate `num_perturbations` perturbed `xs` in the neighborhood of `x`
         - find xs which results in the same label
         - (or use pre-computed)
         - Compute (or use pre-computed) explanations `e_x` and `e_xs`
         - Compute relative representation stability objective, find as subject to `xs`
         - In practise we just use `max` over a finite `xs_batch`

        """
        if not self.disable_warnings:
            warn_func.warn_parameterisation(
                metric_name="Relative Representation Stability",
                sensitive_params="choice which internal representations use 'layer_names', 'layer_indices'",
                citation="Chirag Agarwal, et. al., 2022. \"Rethinking stability for attribution based explanations.\" https://arxiv.org/pdf/2203.06877.pdf"
            )
        channel_first = utils.infer_channel_first(x_batch)
        model_wrapper = utils.get_wrapped_model(model, channel_first)

        if "xs_batch" in kwargs:
            xs_batch = kwargs.get("xs_batch")
            if len(xs_batch.shape) <= len(x_batch.shape):
                raise ValueError("xs_batch must have 1 more batch axis than x_batch")
        else:
            xs_batch = compute_perturbed_inputs_with_same_labels(
                model=model_wrapper,
                x_batch=x_batch,
                y_batch=y_batch,
                num_perturbations=self.num_perturbations,
                display_progressbar=self.display_progressbar,
                **kwargs,
            )
        assert_correct_kwargs_provided(a_batch, **kwargs)
        if a_batch is not None:
            as_batch = kwargs.get("as_batch")
        else:
            # Add xs_batch to kwargs in case it was not provided
            kwargs["xs_batch"] = xs_batch
            a_batch, as_batch = compute_explanations(
                model=model_wrapper,
                x_batch=x_batch,
                y_batch=y_batch,
                normalize=self.normalise,
                absolute=self.abs,
                display_progressbar=self.display_progressbar,
                normalize_func=self.normalise_func,
                **kwargs,
            )

        l_x = model_wrapper.get_hidden_layers_representations(x_batch, **kwargs)

        # "Merge" axis 0,1
        num_perturbations = xs_batch.shape[0]
        batch_size = xs_batch.shape[1]
        model_input = xs_batch.reshape((-1, *xs_batch.shape[2:]))

        l_xs_flat = model_wrapper.get_hidden_layers_representations(
            model_input, **kwargs
        )
        # Un-"merge" axis 0,1
        l_xs = l_xs_flat.reshape((num_perturbations, batch_size, *l_xs_flat.shape[1:]))

        obj_arr = np.asarray(
            [
                self.relative_representation_stability_objective(l_x, i, a_batch, j)
                for i, j in zip(l_xs, as_batch)
            ]
        )
        result = np.max(obj_arr, axis=0)
        if self.return_aggregate:
            result = self.aggregate_func(result)
        return result

    @staticmethod
    def relative_representation_stability_objective(
        l_x: np.ndarray,
        l_xs: np.ndarray,
        e_x: np.ndarray,
        e_xs: np.ndarray,
        eps_min=1e-6,
    ) -> np.ndarray:
        """
        Computes relative representation stabilities maximization objective
        as defined here https://arxiv.org/pdf/2203.06877.pdf by the authors.

        Parameters:
            l_x:  2D tensor of internal representations for x with shape (batch_size, ...)
            l_xs: 2D tensor of internal representations for xs with shape (batch_size, ...)
            e_x:  4D tensor of explanations for x with shape (batch_size, ...)
            e_xs: 4D tensor of explanations for xs with shape (batch_size, ...)
            eps_min: float, prevents division by 0
        Returns:
            rrs_obj: 1D tensor with shape (batch_size,)
        """

        nominator = (e_x - e_xs) / (e_x + (e_x == 0) * eps_min)  # prevent division by 0
        if len(nominator.shape) == 3:
            # In practise quantus.explain often returns tensors of shape (batch_size, img_height, img_width)
            nominator = np.linalg.norm(nominator, axis=(2, 1))  # noqa
        else:
            nominator = np.linalg.norm(
                np.linalg.norm(nominator, axis=(3, 2)), axis=1  # noqa
            )  # noqa

        denominator = l_x - l_xs
        denominator /= l_x + (l_x == 0) * eps_min  # prevent division by 0

        denominator = np.linalg.norm(denominator, axis=1)
        denominator += (denominator == 0) * eps_min

        return nominator / denominator