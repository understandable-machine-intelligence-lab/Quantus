{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"name":"Getting-started.ipynb","provenance":[{"file_id":"1tlXprNNO0PAcdH4Wh5ThFWgknpSQTkmd","timestamp":1626956510892}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"683c3ae126b14059a499e9b6c637056e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_903a76a66ac04d05b055287655847d8b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_befdef7b110940b5aa85a71c19e9551c","IPY_MODEL_2657799fef464bb9bf424f333fe8a71d"]}},"903a76a66ac04d05b055287655847d8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"befdef7b110940b5aa85a71c19e9551c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9bd342dc99934cf395dd7f262b395025","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f252080ca7e74a1eb534a48263d701fc"}},"2657799fef464bb9bf424f333fe8a71d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7cf6782d79f8479d9833f6cc5cdfbee9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [03:47&lt;00:00, 206kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_38775cb19aaa4b8f804aac261a65c7c7"}},"9bd342dc99934cf395dd7f262b395025":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f252080ca7e74a1eb534a48263d701fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7cf6782d79f8479d9833f6cc5cdfbee9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"38775cb19aaa4b8f804aac261a65c7c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"bukochgPFg7s"},"source":["## Getting started\n","\n","This tutorial demonstrates how to use the library in the most simple ways -- to evaluate the outcome of different commonly used explanation methods to better understand how the the prediction model behavious. \n","\n","For this purpose, we use a pre-trained ResNet-18 model and Tiny ImageNet dataset to showcase the library's functionality.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"MuA8v9jLp-pf","colab":{"base_uri":"https://localhost:8080/","height":603},"executionInfo":{"status":"ok","timestamp":1629809030951,"user_tz":-120,"elapsed":111341,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"76dc829a-2cc9-40f2-8360-cc73ad14ba8e"},"source":["# Mount Google Drive. #remove\n","from google.colab import drive \n","drive.mount('/content/drive', force_remount=True)\n","\n","# Install packages. #remove\n","!pip install captum opencv-python torch==1.8.0 torchvision==0.9.0\n","\n","# Imports general.\n","import sys\n","import warnings\n","import gc\n","import pathlib\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","import captum\n","from torchvision import transforms\n","from tqdm import tqdm\n","from captum.attr import *\n","import random\n","\n","# Import package.\n","sys.path.append('/content/drive/MyDrive/Projects/xai_quantification_toolbox')\n","import quantus\n","\n","# Notebook settings.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","%load_ext autoreload\n","%autoreload 2\n","\n","# Collect garbage.\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","Collecting captum\n","  Using cached captum-0.4.0-py3-none-any.whl (1.4 MB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n","Collecting torch==1.8.0\n","  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n","\u001b[K     |████████████████████████████████| 735.5 MB 14 kB/s \n","\u001b[?25hCollecting torchvision==0.9.0\n","  Downloading torchvision-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (17.3 MB)\n","\u001b[K     |████████████████████████████████| 17.3 MB 39 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.19.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.0) (7.1.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from captum) (3.2.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.4.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (1.3.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->captum) (1.15.0)\n","Installing collected packages: torch, torchvision, captum\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.10.0+cu102\n","    Uninstalling torchvision-0.10.0+cu102:\n","      Successfully uninstalled torchvision-0.10.0+cu102\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n","Successfully installed captum-0.4.0 torch-1.8.0 torchvision-0.9.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["torch","torchvision"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"fPmV0AglLds8"},"source":["\n","import quantus\n","import torch\n","import torchvision\n","\n","# Load a pre-trained classification model.\n","model = torchvision.models.resnet18(pretrained=True)\n","model.eval()\n","\n","# Load datasets and make loaders.\n","test_set = torchvision.datasets.Caltech256(root='./sample_data', \n","                                           download=True,\n","                                           transform=torchvision.transforms.Compose([torchvision.transforms.Resize(256),\n","                                                                                     torchvision.transforms.CenterCrop((224, 224)),\n","                                                                                     torchvision.transforms.ToTensor(),\n","                                                                                     torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=12)\n","\n","# Load a batch of inputs and outputs to use for evaluation.\n","x_batch, y_batch = iter(test_loader).next()\n","x_batch, y_batch = x_batch.cpu().numpy(), y_batch.cpu().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mB2QuiaDlu7w"},"source":["### Load Tiny Imagenet dataset."]},{"cell_type":"code","metadata":{"id":"PZ6VyL7x26Ue"},"source":["# Load test data and loaders.\n","test_set = torchvision.datasets.ImageFolder(root='/content/drive/My Drive/imagenet_images',\n","                                            transform=transforms.Compose([transforms.Resize(256),\n","                                                                          transforms.CenterCrop((224, 224)),\n","                                                                          transforms.ToTensor(),\n","                                                                          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n","test_loader = torch.utils.data.DataLoader(test_set, shuffle=True, batch_size=24)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKwN9uWs29sn"},"source":["### Load a pre-trained ResNet-18 model.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["683c3ae126b14059a499e9b6c637056e","903a76a66ac04d05b055287655847d8b","befdef7b110940b5aa85a71c19e9551c","2657799fef464bb9bf424f333fe8a71d","9bd342dc99934cf395dd7f262b395025","f252080ca7e74a1eb534a48263d701fc","7cf6782d79f8479d9833f6cc5cdfbee9","38775cb19aaa4b8f804aac261a65c7c7"]},"id":"ypxtHPfolQPD","executionInfo":{"status":"ok","timestamp":1627477332121,"user_tz":-120,"elapsed":1755,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"445febc3-c334-4746-cbc6-beba49b51ee2"},"source":["# Load pre-trained ResNet18 model.\n","model = torchvision.models.resnet18(pretrained=True)\n","model.eval()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"683c3ae126b14059a499e9b6c637056e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"XqKzag4VFjHT"},"source":["### Load attributions."]},{"cell_type":"code","metadata":{"id":"uSUkm-d6-p20"},"source":["# Load attributions and plot them. \n","a_batch = explain(model, \n","                  x_batch, \n","                  y_batch, \n","                  explanation_func=\"IntegratedGradients\"\n","                  **{\"normalize\": True})\n","\n","# Plot examplary explanations!\n","fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(5, 3))\n","axes[0].imshow(denormalize_image(x_batch[i].cpu(), **{\"img_size\": 32}).transpose(0, 1).transpose(1, 2))\n","exp = axes[1].imshow(a_batch[i], cmap=\"seismic\"); fig.colorbar(exp)\n","axes[0].axis(\"off\"); axes[1].axis(\"off\"); plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vrl4GiojK6r-"},"source":["### Evaluate the attributions."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138},"id":"G3icNiGKZbz8","executionInfo":{"status":"ok","timestamp":1627402923231,"user_tz":-120,"elapsed":32766,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"d915b106-97bb-4537-9b7d-e1b567ac1fc1"},"source":["# One-liner to faithfulness of provided attributions.\n","test = FaithfulnessCorrelation(**{\"subset_size\": 224,})\n","\n","scores = test(model=model, \n","              x_batch=x_batch.cpu().numpy(), \n","              y_batch=y_batch.cpu().numpy(), \n","              a_batch=a_batch.cpu().numpy(), \n","              **{\"explanation_func\": \"IntegratedGradients\", \"device\": device})\n","\n","print(test.HOWTOREADSCORES)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Gradient</th>\n","      <th>InputXGradient</th>\n","      <th>Saliency</th>\n","      <th>GradientShap</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Faithfulness correlation</th>\n","      <td>-0.020065</td>\n","      <td>0.105451</td>\n","      <td>0.032546</td>\n","      <td>0.062868</td>\n","    </tr>\n","    <tr>\n","      <th>max-Sensitivity</th>\n","      <td>0.287529</td>\n","      <td>0.137074</td>\n","      <td>0.219753</td>\n","      <td>0.175271</td>\n","    </tr>\n","    <tr>\n","      <th>Infidelity</th>\n","      <td>15.868220</td>\n","      <td>3.496568</td>\n","      <td>25.415687</td>\n","      <td>2.702603</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                           Gradient  InputXGradient   Saliency  GradientShap\n","Faithfulness correlation  -0.020065        0.105451   0.032546      0.062868\n","max-Sensitivity            0.287529        0.137074   0.219753      0.175271\n","Infidelity                15.868220        3.496568  25.415687      2.702603"]},"metadata":{"tags":[]},"execution_count":184}]},{"cell_type":"code","metadata":{"id":"b3SnLtZc2I13"},"source":[""],"execution_count":null,"outputs":[]}]}