{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"name":"Tutorial-evaluating-explanations-during-model-training.ipynb","provenance":[{"file_id":"1em6hsNlRKEM6-4W5055GUhMGEsiDn32X","timestamp":1627550362887},{"file_id":"1nMiFlKaXP_F5rbeFwc35BL4SwO2ktnEs","timestamp":1627483591348},{"file_id":"14H0YjeULfWNdvzhVJbiScA0sUcLhGmY0","timestamp":1627472772689},{"file_id":"1tlXprNNO0PAcdH4Wh5ThFWgknpSQTkmd","timestamp":1626956510892}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bukochgPFg7s"},"source":["## Tutorial - Evaluate explanations during model training\n","\n","This tutorial demonstrates how one can use the library to evaluate how explanations changes while a model is training. We use a pre-trained AlexNet model and Tiny Imagenet dataset to showcase the library's functionality.\n","\n"]},{"cell_type":"code","metadata":{"id":"MuA8v9jLp-pf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627560323796,"user_tz":-120,"elapsed":7048,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"435f5cd8-24d6-4e92-c961-a246e9d7f0c8"},"source":["# Mount Google Drive. #remove\n","from google.colab import drive \n","drive.mount('/content/drive', force_remount=True)\n","\n","# Install packages. #remove\n","!pip install captum opencv-python torch==1.8.0 torchvision==0.9.0\n","\n","# Imports general.\n","import sys\n","import warnings\n","import gc\n","import pathlib\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","import captum\n","from torchvision import transforms\n","from tqdm import tqdm\n","from captum.attr import *\n","import random\n","\n","# Import package.\n","sys.path.append('/content/drive/MyDrive/Projects/xai_quantification_toolbox')\n","import quantus\n","from quantus.metrics import *\n","\n","# Notebook settings.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","%load_ext autoreload\n","%autoreload 2\n","\n","# Collect garbage.\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: captum in /usr/local/lib/python3.7/dist-packages (0.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from captum) (1.19.5)\n","Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from captum) (1.8.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from captum) (3.2.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->captum) (3.7.4.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->captum) (1.15.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n","Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (1.8.0)\n","Requirement already satisfied: torchvision==0.9.0 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.19.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.0) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mB2QuiaDlu7w"},"source":["### Load 5 classes of Imagenet dataset."]},{"cell_type":"code","metadata":{"id":"PZ6VyL7x26Ue"},"source":["# TODO. Update to tiny imagenet dataset!\n","\n","# Load datasets and make loaders.\n","test_set = torchvision.datasets.ImageFolder(root='/content/drive/My Drive/imagenet_images',\n","                                            transform=transforms.Compose([transforms.Resize(256),\n","                                                                          transforms.CenterCrop((224, 224)),\n","                                                                          transforms.ToTensor(),\n","                                                                          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n","test_loader = torch.utils.data.DataLoader(test_set, shuffle=True, batch_size=56)\n","\n","# Load a batch of inputs and outputs to use for evaluation.\n","x_batch, y_batch = iter(test_loader).next()\n","x_batch, y_batch = x_batch.to(device), y_batch.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKwN9uWs29sn"},"source":["### During model training/ fine-tuning calculate max-sensitivity scores of Integrated Gradients explanations."]},{"cell_type":"code","metadata":{"id":"cpv0QMA3uTMK"},"source":["def evaluate_model(model, images, labels, device):\n","    \"\"\"Evaluate torch model given images and lables and return predictions and targets.\"\"\"\n","    model.eval()\n","    logits = torch.Tensor().to(device)\n","    targets = torch.LongTensor().to(device)    \n","    return torch.nn.functional.softmax(torch.cat([logits, model(images)]), dim=1), torch.cat([targets, labels])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lbfAkSEtmGym","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627563847888,"user_tz":-120,"elapsed":1046525,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"06c57a44-3cf5-4c77-bc01-c78c8902538c"},"source":["# Load AlexNet model (only constructor, not with weights).\n","model = torchvision.models.mobilenet_v3_small()\n","\n","# Set necessary configs/ parameters.\n","model.to(device)  \n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","path_model_weights = \"drive/MyDrive/Projects/quantus/nbs/resources/imagenet5\"\n","epochs = 5\n","nr_samples = 56\n","max_batches = 12\n","sensitivities = {}\n"," \n","for epoch in range(epochs):\n","    model.train() \n","    \n","    for b, (images, labels) in enumerate(test_loader):\n","        \n","        if b >= max_batches:\n","            break\n","\n","        images, labels = images.to(device), labels.to(device)\n","        logits = model(images)\n","\n","        loss = criterion(logits, labels)\n","        model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Evaluate model!\n","    predictions, labels = evaluate_model(model, x_batch.to(device), y_batch.to(device), device)\n","    test_acc = np.mean(np.argmax(predictions.cpu().detach().numpy(), axis=1) == labels.detach().cpu().numpy())\n","    \n","    # Explain model (on a few test samples) and measure sensitivies.\n","    sensitivities[epoch] = MaxSensitivity()(model=model, \n","                                            x_batch=x_batch[:nr_samples].cpu().numpy(), \n","                                            y_batch=y_batch[:nr_samples].cpu().numpy(), \n","                                            a_batch=None, \n","                                            **{\"explanation_func\": \"Saliency\", \n","                                                \"device\": device,\n","                                                \"img_size\": 224})\n","    \n","    print(f\"Epoch {epoch+1}/{epochs} - train accuracy: {(100 * test_acc):.2f}% - max sensitivity {np.mean(sensitivities[epoch]):.2f}\")\n","\n","# Save model.\n","torch.save(model.state_dict(), path_model_weights)\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5 - train accuracy: 30.36% - max sensitivity 0.03\n","Epoch 2/5 - train accuracy: 30.36% - max sensitivity 0.05\n","Epoch 3/5 - train accuracy: 30.36% - max sensitivity 0.05\n","Epoch 4/5 - train accuracy: 30.36% - max sensitivity 0.04\n","Epoch 5/5 - train accuracy: 30.36% - max sensitivity 0.02\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["MobileNetV3(\n","  (features): Sequential(\n","    (0): ConvBNActivation(\n","      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","      (2): Hardswish()\n","    )\n","    (1): InvertedResidual(\n","      (block): Sequential(\n","        (0): ConvBNActivation(\n","          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n","          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (1): SqueezeExcitation(\n","          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): ConvBNActivation(\n","          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Identity()\n","        )\n","      )\n","    )\n","    (2): InvertedResidual(\n","      (block): Sequential(\n","        (0): ConvBNActivation(\n","          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (1): ConvBNActivation(\n","          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n","          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (2): ConvBNActivation(\n","          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Identity()\n","        )\n","      )\n","    )\n","    (3): InvertedResidual(\n","      (block): Sequential(\n","        (0): ConvBNActivation(\n","          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (1): ConvBNActivation(\n","          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n","          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (2): ConvBNActivation(\n","          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Identity()\n","        )\n","      )\n","    )\n","    (4): InvertedResidual(\n","      (block): Sequential(\n","        (0): ConvBNActivation(\n","          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): ConvBNActivation(\n","          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): ConvBNActivation(\n","          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Identity()\n","        )\n","      )\n","    )\n","    (5): InvertedResidual(\n","      (block): Sequential(\n","        (0): ConvBNActivation(\n","          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): ConvBNActivation(\n","          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n","          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): ConvBNActivation(\n","          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Identity()\n","        )\n","      )\n","    )\n","    (6): InvertedResidual(\n","      (block): Sequential(\n","        (0): ConvBNActivation(\n","          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): ConvBNActivation(\n","          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n","          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): ConvBNActivation(\n","          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Identity()\n","        )\n","      )\n","    )\n","    (7): InvertedResidual(\n","      (block): Sequential(\n","        (0): ConvBNActivation(\n","          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): ConvBNActivation(\n","          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n","          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): ConvBNActivation(\n","          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Identity()\n","        )\n","      )\n","    )\n","    (8): InvertedResidual(\n","      (block): Sequential(\n","        (0): ConvBNActivation(\n","          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): ConvBNActivation(\n","          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n","          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): ConvBNActivation(\n","          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Identity()\n","        )\n","      )\n","    )\n","    (9): InvertedResidual(\n","      (block): Sequential(\n","        (0): ConvBNActivation(\n","          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): ConvBNActivation(\n","          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n","          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): ConvBNActivation(\n","          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Identity()\n","        )\n","      )\n","    )\n","    (10): InvertedResidual(\n","      (block): Sequential(\n","        (0): ConvBNActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): ConvBNActivation(\n","          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): ConvBNActivation(\n","          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Identity()\n","        )\n","      )\n","    )\n","    (11): InvertedResidual(\n","      (block): Sequential(\n","        (0): ConvBNActivation(\n","          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (1): ConvBNActivation(\n","          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n","          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Hardswish()\n","        )\n","        (2): SqueezeExcitation(\n","          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n","          (relu): ReLU(inplace=True)\n","          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): ConvBNActivation(\n","          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","          (2): Identity()\n","        )\n","      )\n","    )\n","    (12): ConvBNActivation(\n","      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n","      (2): Hardswish()\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=1)\n","  (classifier): Sequential(\n","    (0): Linear(in_features=576, out_features=1024, bias=True)\n","    (1): Hardswish()\n","    (2): Dropout(p=0.2, inplace=True)\n","    (3): Linear(in_features=1024, out_features=1000, bias=True)\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7jjJFvJm_BsB","executionInfo":{"status":"ok","timestamp":1627557470179,"user_tz":-120,"elapsed":581,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"6044f1d4-4c15-4926-e9f3-d4d45a6ba1c2"},"source":["[np.mean(v) for k, v in sensitivities]\n","\n","# Summarise in a dataframe.      \n","df = pd.DataFrame(sensitivities)\n","df[\"avg\"] = df.mean(axis=0)\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.03605495,\n"," 0.038111627,\n"," 0.035774343,\n"," 0.036996625,\n"," 0.03895698,\n"," 0.035916544,\n"," 0.025491755,\n"," 0.0071537965,\n"," 0.022229657,\n"," 0.0200718,\n"," 0.04412011,\n"," 0.032924026,\n"," 0.043629825,\n"," 0.021562316,\n"," 0.01864284,\n"," 0.027031252,\n"," 0.020951644,\n"," 0.0147696175,\n"," 0.027309624,\n"," 0.034381274,\n"," 0.037629068,\n"," 0.017584994,\n"," 0.025542881,\n"," 0.018253563]"]},"metadata":{"tags":[]},"execution_count":36}]}]}