{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This notebook shows the functionality of the LocalizationTest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/motzkus/work/xai-quantification-toolbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from xai_quantification_toolbox.measures.localization_test import *\n",
    "from xai_quantification_toolbox.loaders.model_interface import *\n",
    "#from xai_quantification_toolbox.quantifier.base import *\n",
    "#from ....NoiseGrad.src.models import ResNet18\n",
    "\n",
    "\n",
    "\n",
    "#!pip install captum\n",
    "#!pip install opencv-python\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "from xml.etree import ElementTree\n",
    "import xmltodict\n",
    "import cv2\n",
    "#import h5py\n",
    "#from tqdm import tqdm\n",
    "from captum.attr import Saliency, IntegratedGradients\n",
    "#from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Retrieve source code.\n",
    "#from drive.MyDrive.Projects.xai_quantification_toolbox import * #import xaiquantificationtoolbox\n",
    "\n",
    "# Notebook settings.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Load model, data and attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained vgg16 model.\n",
    "model = torchvision.models.vgg16(pretrained=False)\n",
    "model.classifier[-1] = torch.nn.Linear(4096, 20)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"../../../xai_discriminability/models/pytorch/vgg16_voc/model.pt\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2012Sample:\n",
    "    \"\"\" Implements a pascal voc 2012 sample. \"\"\"\n",
    "\n",
    "    def __init__(self, datum, filename, label, one_hot_label, binary_mask):\n",
    "        self.datum = datum\n",
    "        self.filename = filename\n",
    "        self.label = label\n",
    "        self.one_hot_label = one_hot_label\n",
    "        self.binary_mask = binary_mask\n",
    "\n",
    "class VOC2012Dataset:\n",
    "    \"\"\" Implements the pascal voc 2012 dataset. \"\"\"\n",
    "\n",
    "    def __init__(self, datapath, partition, classidx=None):\n",
    "        \"\"\" Initialize pascal voc 2012 dataset. \"\"\"\n",
    "        #super().__init__(datapath, partition)\n",
    "        self.datapath = datapath\n",
    "        self.partition = partition\n",
    "        self.samples = []\n",
    "\n",
    "        self.cmap = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "                     'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\n",
    "                     'tvmonitor']\n",
    "        if not classidx:\n",
    "            self.classes = self.cmap\n",
    "\n",
    "        else:\n",
    "            self.classes = []\n",
    "            for idx in classidx:\n",
    "                self.classes.append(self.cmap[int(idx)])\n",
    "\n",
    "        self.labels = []\n",
    "\n",
    "        if not classidx:\n",
    "            f = open(datapath + \"ImageSets/Main/\" + partition + \".txt\", \"r\")\n",
    "        else:\n",
    "            f = []\n",
    "            for idx in classidx:\n",
    "                with open(datapath + \"ImageSets/Main/\" + self.cmap[int(idx)] + \"_\" + partition + \".txt\", \"r\") as classfile:\n",
    "                    for line in classfile:\n",
    "                        filename, in_class = [value for value in line.split(\" \") if value]\n",
    "                        if in_class.startswith(\"1\") and (filename not in f):\n",
    "                            f.append(filename)\n",
    "\n",
    "        for line in f:\n",
    "            if line.endswith(\"\\n\"):\n",
    "                line = line[:-1]\n",
    "            # get image filepath\n",
    "            self.samples.append(datapath + \"JPEGImages/\" + line + \".jpg\")\n",
    "\n",
    "            # parse annotations\n",
    "            tree = ElementTree.parse(datapath + \"Annotations/\" + line + \".xml\")\n",
    "            xml_data = tree.getroot()\n",
    "            xmlstr = ElementTree.tostring(xml_data, encoding=\"utf-8\", method=\"xml\")\n",
    "            annotation = dict(xmltodict.parse(xmlstr))['annotation']\n",
    "\n",
    "            objects = annotation[\"object\"]\n",
    "\n",
    "            if type(objects) != list:\n",
    "                self.labels.append([objects['name']])\n",
    "\n",
    "            else:\n",
    "                label = []\n",
    "                for object in annotation['object']:\n",
    "                    if type(object) == collections.OrderedDict:\n",
    "                        if object['name'] not in label:\n",
    "                            label.append(object['name'])\n",
    "\n",
    "                self.labels.append(label)\n",
    "        \n",
    "        print(\"{} samples loaded\".format(len(self.samples)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get the datapoint at index. \"\"\"\n",
    "\n",
    "        filename = self.samples[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        image = self.preprocess_image(filename)\n",
    "        one_hot_label = self.preprocess_label(label)\n",
    "        binary_mask = self.preprocess_binary_mask(filename)\n",
    "\n",
    "        sample = VOC2012Sample(\n",
    "            image,\n",
    "            filename,\n",
    "            label,\n",
    "            one_hot_label,\n",
    "            binary_mask\n",
    "        )\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def classname_to_idx(self, class_name):\n",
    "        \"\"\" convert a classname to an index. \"\"\"\n",
    "        return self.cmap.index(class_name)\n",
    "\n",
    "    def preprocess_image(self, image):\n",
    "\n",
    "        read_image = cv2.imread(image, cv2.IMREAD_COLOR)\n",
    "        image_resized = cv2.resize(read_image, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "        image_normalized = image_resized.astype(np.float32) / 127.5 - 1.0\n",
    "\n",
    "        return image_normalized\n",
    "\n",
    "    def preprocess_label(self, label):\n",
    "        \"\"\" Convert label to one hot encoding. \"\"\"\n",
    "        one_hot_label = np.zeros(len(self.cmap))\n",
    "\n",
    "        for classname in label:\n",
    "            one_hot_label[self.cmap.index(classname)] = 1\n",
    "\n",
    "        return one_hot_label\n",
    "\n",
    "    def preprocess_binary_mask(self, filename):\n",
    "        \"\"\" Get the bounding box as binary mask.\"\"\"\n",
    "\n",
    "        binary_mask = {}\n",
    "        #filename = extract_filename(filename)\n",
    "        filename = filename.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        # parse annotations\n",
    "        tree = ElementTree.parse(self.datapath + \"Annotations/\" + filename + \".xml\")\n",
    "        xml_data = tree.getroot()\n",
    "        xmlstr = ElementTree.tostring(xml_data, encoding=\"utf-8\", method=\"xml\")\n",
    "        annotation = dict(xmltodict.parse(xmlstr))['annotation']\n",
    "\n",
    "        width = int(annotation[\"size\"][\"width\"])\n",
    "        height = int(annotation[\"size\"][\"height\"])\n",
    "\n",
    "        # iterate objects\n",
    "        objects = annotation[\"object\"]\n",
    "\n",
    "        if type(objects) != list:\n",
    "            # self.labels.append([objects['name']])\n",
    "            mask = np.zeros((width, height), dtype=int)\n",
    "\n",
    "            mask[int(objects['bndbox']['xmin']):int(objects['bndbox']['xmax']), int(objects['bndbox']['ymin']):int(objects['bndbox']['ymax'])] = 1\n",
    "\n",
    "            binary_mask[objects['name']] = mask\n",
    "\n",
    "        else:\n",
    "            for object in annotation['object']:\n",
    "                if type(object) == collections.OrderedDict:\n",
    "                    if object['name'] in binary_mask.keys():\n",
    "                        mask = binary_mask[object['name']]\n",
    "                    else:\n",
    "                        mask = np.zeros((width, height), dtype=np.uint8)\n",
    "\n",
    "                    mask[int(object['bndbox']['xmin']):int(object['bndbox']['xmax']), int(object['bndbox']['ymin']):int(object['bndbox']['ymax'])] = 1\n",
    "\n",
    "                    binary_mask[object['name']] = mask\n",
    "\n",
    "        # preprocess binary masks to fit shape of image data\n",
    "        for key in binary_mask.keys():\n",
    "            # binary_mask[key] = tf.image.resize(binary_mask[key][:, :, np.newaxis], [224, 224]).numpy().astype(int)\n",
    "            binary_mask[key] = cv2.resize(binary_mask[key], (224, 224), interpolation=cv2.INTER_NEAREST).astype(np.int)[:, :, np.newaxis]\n",
    "\n",
    "        return binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341 samples loaded\n"
     ]
    }
   ],
   "source": [
    "classidx = 4\n",
    "\n",
    "dataset = VOC2012Dataset(\"../../../data/VOC2012/\", \"val\", classidx=[classidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [dataset[i] for i in range(10)]\n",
    "#x_batch, y_batch, a_batch, s_batch\n",
    "#a_batch = explain(model, x_batch.to(device), y_batch.to(device), explanation_func=\"Saliency\")\n",
    "x_batch = np.array([sample.datum for sample in data])\n",
    "y_batch = np.array([sample.one_hot_label for sample in data])\n",
    "s_batch = np.array([sample.binary_mask[dataset.cmap[classidx]] for sample in data])[:, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a_batch_saliency = explain(model.to(device), x_batch, classidx, explanation_func=\"Saliency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 224, 224])\n",
      "(10, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "print(a_batch_saliency.shape)\n",
    "a_batch_saliency = a_batch_saliency.cpu().numpy()\n",
    "\n",
    "print(s_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Option 1. Evaluate the localization authenticity of attributions in one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# One-liner to measure pointing game results of provided attributions.\n",
    "scores = PointingGame()(model=model, x_batch=x_batch, y_batch=y_batch, a_batch=a_batch_saliency, s_batch=s_batch, device=device, **{\"explanation_func\": \"Saliency\"})\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.061176937, 0.003850803, 0.018578487, 0.004330946, 0.03572835, 0.03354629, 0.0054374468, 0.021349076, 0.013994433, 0.010142415]\n"
     ]
    }
   ],
   "source": [
    "# One-liner to measure the attribution localization results of provided attributions.\n",
    "scores = AttributionLocalization()(model=model, x_batch=x_batch, y_batch=y_batch, a_batch=a_batch_saliency, s_batch=s_batch, device=device, **{\"explanation_func\": \"Saliency\"})\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner to measure the top-k intersection results of provided attributions.\n",
    "scores = TopKIntersection()(model=model, x_batch=x_batch, y_batch=y_batch, a_batch=a_batch_saliency, s_batch=s_batch, device=device, **{\"explanation_func\": \"Saliency\"})\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner to measure the relevance rank accuracy results of provided attributions.\n",
    "scores = RelevanceRankAccuracy()(model=model, x_batch=x_batch, y_batch=y_batch, a_batch=a_batch_saliency, s_batch=s_batch, device=device, **{\"explanation_func\": \"Saliency\"})\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Option 2. Evaluate the robustness of provided attributions while enjoying more functionality of Quantifier and Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-40298831611c>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-40298831611c>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    quantifier = Quantifier(measures=tests, io_object=h5py.File(\"PATH_TO_H5PY_FILE\"), checkpoints=..)\u001b[0m\n\u001b[0m                                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Specify the tests.\n",
    "tests = [RobustnessTest(**{\n",
    "    \"similarity_function\": similarity_fn,\n",
    "    \"perturbation_function\": gaussian_blur,\n",
    "}) for similarity_fn in [lipschitz_constant, distance_euclidean, cosine]]\n",
    "\n",
    "# Load attributions of another explanation method.\n",
    "a_batch_intgrad = IntegratedGradients(model).attribute(inputs=x_batch, targets=y_batch)\n",
    "\n",
    "# Init the quantifier object.\n",
    "quantifier = Quantifier(measures=tests, io_object=h5py.File(\"PATH_TO_H5PY_FILE\"), checkpoints=..)\n",
    "\n",
    "# Score the tests.\n",
    "results = [quantifier.score(model=model, x_batch=x_batch, y_batch=y_batch, a_batch=a_batch)\n",
    "           for a_batch in [a_batch_saliency, a_batch_intgrad]]\n",
    "\n",
    "# Plot Saliency vs Integrated Gradients.\n",
    "Plotting(results, show=False, path_to_save=\"PATH_TO_SAVE_FIGURE\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "separability",
   "language": "python",
   "name": "separability"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
