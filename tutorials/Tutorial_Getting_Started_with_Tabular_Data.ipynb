{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bukochgPFg7s"
   },
   "source": [
    "# Getting Started with tabular data!\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/understandable-machine-intelligence-lab/Quantus/main?labpath=tutorials%2FTutorial_Getting_Started_with_Tabular_Data.ipynb)\n",
    "\n",
    "\n",
    "This notebook shows how to get started with Quantus using tabular data. For this purpose, we use the classic Titanic tabular dataset (Frank E. Harrell Jr., Thomas Cason):\n",
    "\n",
    "https://www.openml.org/d/40945\n",
    "\n",
    "The model in this notebook is taken from \"Getting started with Captum - Titanic Data Analysis\" provided by Captum:\n",
    "\n",
    "https://captum.ai/tutorials/Titanic_Basic_Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4Y7_mNf9Bic0"
   },
   "outputs": [],
   "source": [
    "!pip install quantus torch captum tensorflow-datasets\n",
    " \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RV7X-Ss9-16F"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.image import grayscale_to_rgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import quantus\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(27)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "np.random.seed(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGhP4bTuoWYF"
   },
   "source": [
    "## 1) Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqKzag4VFjHT"
   },
   "source": [
    "### 1.1 Load datasets\n",
    "\n",
    "We load the dataset using the tensorflow-datasets library. Alternatively, it can be downloaded directly from the OpenML website: https://www.openml.org/d/40945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TmsZxFhuc0mm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 13:58:56.177374: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "(ds, _), ds_info = tfds.load(\n",
    "    'titanic',\n",
    "    split=[\"train\", \"train[:1]\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "df = tfds.as_dataframe(ds, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['features/age', 'features/embarked', 'features/fare', 'features/parch', 'features/pclass', 'features/sex',\n",
    "       'features/sibsp', 'survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features/age</th>\n",
       "      <th>features/embarked</th>\n",
       "      <th>features/fare</th>\n",
       "      <th>features/parch</th>\n",
       "      <th>features/pclass</th>\n",
       "      <th>features/sex</th>\n",
       "      <th>features/sibsp</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1309.00000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "      <td>1309.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.67660</td>\n",
       "      <td>1.495034</td>\n",
       "      <td>33.269279</td>\n",
       "      <td>0.385027</td>\n",
       "      <td>1.294882</td>\n",
       "      <td>0.355997</td>\n",
       "      <td>0.498854</td>\n",
       "      <td>0.381971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.86619</td>\n",
       "      <td>0.816130</td>\n",
       "      <td>51.747562</td>\n",
       "      <td>0.865560</td>\n",
       "      <td>0.837836</td>\n",
       "      <td>0.478997</td>\n",
       "      <td>1.041658</td>\n",
       "      <td>0.486055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.895800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>35.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>31.275000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.00000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>512.329224</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       features/age  features/embarked  features/fare  features/parch  \\\n",
       "count    1309.00000        1309.000000    1309.000000     1309.000000   \n",
       "mean       23.67660           1.495034      33.269279        0.385027   \n",
       "std        17.86619           0.816130      51.747562        0.865560   \n",
       "min        -1.00000           0.000000      -1.000000        0.000000   \n",
       "25%         7.00000           1.000000       7.895800        0.000000   \n",
       "50%        24.00000           2.000000      14.454200        0.000000   \n",
       "75%        35.00000           2.000000      31.275000        0.000000   \n",
       "max        80.00000           3.000000     512.329224        9.000000   \n",
       "\n",
       "       features/pclass  features/sex  features/sibsp     survived  \n",
       "count      1309.000000   1309.000000     1309.000000  1309.000000  \n",
       "mean          1.294882      0.355997        0.498854     0.381971  \n",
       "std           0.837836      0.478997        1.041658     0.486055  \n",
       "min           0.000000      0.000000        0.000000     0.000000  \n",
       "25%           1.000000      0.000000        0.000000     0.000000  \n",
       "50%           2.000000      0.000000        0.000000     0.000000  \n",
       "75%           2.000000      1.000000        1.000000     1.000000  \n",
       "max           2.000000      1.000000        8.000000     1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical variables\n",
    "df_enc = pd.get_dummies(df, columns = ['features/embarked', 'features/pclass', 'features/sex']).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas dataframes to numpy arrays\n",
    "X = df_enc.drop(['survived'], axis=1).values\n",
    "Y = df_enc[\"survived\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test set\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X, Y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmccxpA0n6MY"
   },
   "source": [
    "### 1.2 Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is based on \"Getting started with Captum - Titanic Data Analysis\" provided by Captum:\n",
    "\n",
    "https://captum.ai/tutorials/Titanic_Basic_Interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicSimpleNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(13, 12)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(12, 8)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.linear3 = nn.Linear(8, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lin1_out = self.linear1(x)\n",
    "        sigmoid_out1 = self.sigmoid1(lin1_out)\n",
    "        sigmoid_out2 = self.sigmoid2(self.linear2(sigmoid_out1))\n",
    "        return self.softmax(self.linear3(sigmoid_out2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 => Loss: 0.70\n",
      "Epoch 21/200 => Loss: 0.56\n",
      "Epoch 41/200 => Loss: 0.51\n",
      "Epoch 61/200 => Loss: 0.49\n",
      "Epoch 81/200 => Loss: 0.48\n",
      "Epoch 101/200 => Loss: 0.48\n",
      "Epoch 121/200 => Loss: 0.47\n",
      "Epoch 141/200 => Loss: 0.46\n",
      "Epoch 161/200 => Loss: 0.47\n",
      "Epoch 181/200 => Loss: 0.46\n"
     ]
    }
   ],
   "source": [
    "net = TitanicSimpleNNModel()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 200\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.1)\n",
    "input_tensor = torch.from_numpy(train_features).type(torch.FloatTensor)\n",
    "label_tensor = torch.from_numpy(train_labels)\n",
    "for epoch in range(num_epochs):    \n",
    "    output = net(input_tensor)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print ('Epoch {}/{} => Loss: {:.2f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8482532751091703\n"
     ]
    }
   ],
   "source": [
    "out_probs = net(input_tensor).detach().numpy()\n",
    "out_classes = np.argmax(out_probs, axis=1)\n",
    "print(\"Train Accuracy:\", sum(out_classes == train_labels) / len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7786259541984732\n"
     ]
    }
   ],
   "source": [
    "test_input_tensor = torch.from_numpy(test_features).type(torch.FloatTensor)\n",
    "out_probs = net(test_input_tensor).detach().numpy()\n",
    "out_classes = np.argmax(out_probs, axis=1)\n",
    "print(\"Test Accuracy:\", sum(out_classes == test_labels) / len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vY9mZQanaxr"
   },
   "source": [
    "### 1.3 Generate explanations\n",
    "\n",
    "In this example, we rely on the `captum` library. We use the Integrated Gradients method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_tensor.requires_grad_()\n",
    "attr, delta = ig.attribute(test_input_tensor,target=1, return_convergence_delta=True)\n",
    "attr = attr.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuBkEBv3mihR"
   },
   "source": [
    "## 2) Quantative evaluation using Quantus\n",
    "\n",
    "We can evaluate our explanations on a variety of quantuative criteria but as a motivating example we test the ModelParameterRandomisation scores by Adebayo et al., 2018. This metric measures the distance between the original attribution and a newly computed attribution throughout the process of cascadingly/independently randomizing the model parameters of one layer at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "aLjrKsT6mS9X"
   },
   "outputs": [],
   "source": [
    "# Define metric for evaluation.\n",
    "metric_init = quantus.ModelParameterRandomisation(\n",
    "    similarity_func=quantus.similarity_func.correlation_spearman,\n",
    "    return_sample_correlation=True,\n",
    "    return_aggregate=True,\n",
    "    aggregate_func=np.mean,\n",
    "    layer_order=\"independent\",\n",
    "    disable_warnings=True,\n",
    "    normalise=True,\n",
    "    abs=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "iq7qqDfSmIdj"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a_batch and x_batch must have same number of batches (1 != 393)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [34]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Return ModelParameterRandomisation scores for Integrated Gradients.\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m scores_intgrad \u001B[38;5;241m=\u001B[39m \u001B[43mmetric_init\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m                            \u001B[49m\u001B[43my_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m                            \u001B[49m\u001B[43ma_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mexplain_func\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquantus\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexplain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mexplain_func_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmethod\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mIntegratedGradients\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreduce_axes\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/UMI_Lab/Quantus/quantus/metrics/randomisation/model_parameter_randomisation.py:244\u001B[0m, in \u001B[0;36mModelParameterRandomisation.__call__\u001B[0;34m(self, model, x_batch, y_batch, a_batch, s_batch, channel_first, explain_func, explain_func_kwargs, model_predict_kwargs, softmax, device, batch_size, custom_batch, **kwargs)\u001B[0m\n\u001B[1;32m    241\u001B[0m warn\u001B[38;5;241m.\u001B[39mdeprecation_warnings(kwargs)\n\u001B[1;32m    242\u001B[0m warn\u001B[38;5;241m.\u001B[39mcheck_kwargs(kwargs)\n\u001B[0;32m--> 244\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgeneral_preprocess\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    246\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    247\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[43m    \u001B[49m\u001B[43ma_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43ma_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[43m    \u001B[49m\u001B[43ms_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43ms_batch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    250\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcustom_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchannel_first\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchannel_first\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexplain_func\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexplain_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexplain_func_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexplain_func_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_predict_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_predict_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43msoftmax\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msoftmax\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    258\u001B[0m model \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    259\u001B[0m x_batch \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/Documents/UMI_Lab/Quantus/quantus/metrics/base.py:391\u001B[0m, in \u001B[0;36mMetric.general_preprocess\u001B[0;34m(self, model, x_batch, y_batch, a_batch, s_batch, channel_first, explain_func, explain_func_kwargs, model_predict_kwargs, softmax, device, custom_batch)\u001B[0m\n\u001B[1;32m    383\u001B[0m     a_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplain_func(\n\u001B[1;32m    384\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel\u001B[38;5;241m.\u001B[39mget_model(),\n\u001B[1;32m    385\u001B[0m         inputs\u001B[38;5;241m=\u001B[39mx_batch,\n\u001B[1;32m    386\u001B[0m         targets\u001B[38;5;241m=\u001B[39my_batch,\n\u001B[1;32m    387\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexplain_func_kwargs,\n\u001B[1;32m    388\u001B[0m     )\n\u001B[1;32m    390\u001B[0m \u001B[38;5;66;03m# Expand attributions to input dimensionality.\u001B[39;00m\n\u001B[0;32m--> 391\u001B[0m a_batch \u001B[38;5;241m=\u001B[39m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexpand_attribution_channel\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma_batch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;66;03m# Asserts.\u001B[39;00m\n\u001B[1;32m    394\u001B[0m asserts\u001B[38;5;241m.\u001B[39massert_attributions(x_batch\u001B[38;5;241m=\u001B[39mx_batch, a_batch\u001B[38;5;241m=\u001B[39ma_batch)\n",
      "File \u001B[0;32m~/Documents/UMI_Lab/Quantus/quantus/helpers/utils.py:695\u001B[0m, in \u001B[0;36mexpand_attribution_channel\u001B[0;34m(a_batch, x_batch)\u001B[0m\n\u001B[1;32m    679\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    680\u001B[0m \u001B[38;5;124;03mExpand additional channel dimension(s) for attributions if needed.\u001B[39;00m\n\u001B[1;32m    681\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    692\u001B[0m \u001B[38;5;124;03m    A x_batch with dimensions matching those of a_batch.\u001B[39;00m\n\u001B[1;32m    693\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    694\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m a_batch\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m x_batch\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n\u001B[0;32m--> 695\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    696\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma_batch and x_batch must have same number of batches (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00ma_batch\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m != \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx_batch\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    697\u001B[0m     )\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m a_batch\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m x_batch\u001B[38;5;241m.\u001B[39mndim:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    700\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma must not have greater ndim than x (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00ma_batch\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m > \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx_batch\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    701\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: a_batch and x_batch must have same number of batches (1 != 393)"
     ]
    }
   ],
   "source": [
    "# Return ModelParameterRandomisation scores for Integrated Gradients.\n",
    "scores_intgrad = metric_init(model=net, \n",
    "                            x_batch=test_features,\n",
    "                            y_batch=test_labels,\n",
    "                            a_batch=None,\n",
    "                            explain_func=quantus.explain,\n",
    "                            explain_func_kwargs={\"method\": \"IntegratedGradients\", \"reduce_axes\": ()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1665171594087,
     "user": {
      "displayName": "Anna Hedström",
      "userId": "05540180366077551505"
     },
     "user_tz": -120
    },
    "id": "3kBrG51Lpuq9",
    "outputId": "5de8efa5-03ac-433d-ec7b-ead66740a545"
   },
   "outputs": [],
   "source": [
    "print(f\"ModelParameterRandomisation scores by Adebayo et al., 2018\\n\"       \n",
    "      f\"\\n • Integrated Gradient = \",scores_intgrad)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}