{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:18:13.872303Z",
     "start_time": "2023-03-30T19:18:13.868459Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n",
       " LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import NamedTuple, List\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import normalizers\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "from helpers.model.text_model import R\n",
    "from quantus.helpers.tf_utils import is_xla_compatible_model\n",
    "from quantus.helpers.model.tf_model import TFModelRandomizer\n",
    "from quantus.helpers.model.text_model import TextClassifier\n",
    "from quantus.helpers.model.text_model import Tokenizer as Q_Tokenizer\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "tf.config.list_logical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:18:17.602872Z",
     "start_time": "2023-03-30T19:18:14.923988Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset sst2 (/Users/artemsereda/.cache/huggingface/datasets/sst2/default/2.0.0/9896208a8d85db057ac50c72282bcb8fe755accc671a57dd8059d4e130961ed5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208da842676046e58445eed94466aed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sst2\")\n",
    "\n",
    "X_train = dataset[\"train\"][\"sentence\"]\n",
    "X_val = dataset[\"validation\"][\"sentence\"]\n",
    "\n",
    "Y_train = dataset[\"train\"][\"label\"]\n",
    "Y_val = dataset[\"validation\"][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:21:52.053464Z",
     "start_time": "2023-03-30T19:21:51.223467Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=10_000)\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")\n",
    "\n",
    "tokenizer.train_from_iterator(X_train + X_val, trainer)\n",
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:21:56.837259Z",
     "start_time": "2023-03-30T19:21:56.224551Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.enable_padding()\n",
    "tokenizer.enable_truncation(max_length=30)\n",
    "X_train_encoded = [i.ids for i in tokenizer.encode_batch(X_train)]\n",
    "X_val_encoded = [i.ids for i in tokenizer.encode_batch(X_val)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:22:01.825637Z",
     "start_time": "2023-03-30T19:22:00.235589Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_train_encoded, Y_train)\n",
    ").shuffle(100).batch(2048, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (X_val_encoded, Y_val)\n",
    ").shuffle(100).batch(2048, drop_remainder=True).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:22:02.820398Z",
     "start_time": "2023-03-30T19:22:02.596793Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FNetConfig(NamedTuple):\n",
    "    embedding_dim = 128\n",
    "    intermediate_dim = 256\n",
    "    num_encoder_blocks = 3\n",
    "    max_sequence_length = 30\n",
    "    vocab_size = 10_000\n",
    "    num_labels = 20\n",
    "\n",
    "\n",
    "def clone_initializer(initializer):\n",
    "    \"\"\"Clones an initializer to ensure a new seed.\n",
    "\n",
    "    As of tensorflow 2.10, we need to clone user passed initializers when\n",
    "    invoking them twice to avoid creating the same randomized initialization.\n",
    "    \"\"\"\n",
    "    # If we get a string or dict, just return as we cannot and should not clone.\n",
    "    if not isinstance(initializer, keras.initializers.Initializer):\n",
    "        return initializer\n",
    "    config = initializer.get_config()\n",
    "    return initializer.__class__.from_config(config)\n",
    "\n",
    "\n",
    "\n",
    "class PositionEmbedding(keras.layers.Layer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_length,\n",
    "        initializer=\"glorot_uniform\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        if sequence_length is None:\n",
    "            raise ValueError(\n",
    "                \"`sequence_length` must be an Integer, received `None`.\"\n",
    "            )\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.initializer = keras.initializers.get(initializer)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"initializer\": keras.initializers.serialize(self.initializer),\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        feature_size = input_shape[-1]\n",
    "        self.position_embeddings = self.add_weight(\n",
    "            \"embeddings\",\n",
    "            shape=[self.sequence_length, feature_size],\n",
    "            initializer=self.initializer,\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, start_index=0, **kwargs):\n",
    "        if isinstance(inputs, tf.RaggedTensor):\n",
    "            bounding_shape = inputs.bounding_shape()\n",
    "            position_embeddings = self._trim_and_broadcast_position_embeddings(\n",
    "                bounding_shape,\n",
    "                start_index,\n",
    "            )\n",
    "            # then apply row lengths to recreate the same ragged shape as inputs\n",
    "            return tf.RaggedTensor.from_tensor(\n",
    "                position_embeddings,\n",
    "                inputs.nested_row_lengths(),\n",
    "            )\n",
    "        else:\n",
    "            return self._trim_and_broadcast_position_embeddings(\n",
    "                tf.shape(inputs),\n",
    "                start_index,\n",
    "            )\n",
    "\n",
    "    def _trim_and_broadcast_position_embeddings(self, shape, start_index):\n",
    "        feature_length = shape[-1]\n",
    "        sequence_length = shape[-2]\n",
    "        # trim to match the length of the input sequence, which might be less\n",
    "        # than the sequence_length of the layer.\n",
    "        position_embeddings = tf.slice(\n",
    "            self.position_embeddings,\n",
    "            (start_index, 0),\n",
    "            (sequence_length, feature_length),\n",
    "        )\n",
    "        # then broadcast to add the missing dimensions to match \"shape\"\n",
    "        return tf.broadcast_to(position_embeddings, shape)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(keras.layers.Layer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size,\n",
    "        sequence_length,\n",
    "        embedding_dim,\n",
    "        embeddings_initializer=\"glorot_uniform\",\n",
    "        mask_zero=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        if vocabulary_size is None:\n",
    "            raise ValueError(\n",
    "                \"`vocabulary_size` must be an Integer, received `None`.\"\n",
    "            )\n",
    "        if sequence_length is None:\n",
    "            raise ValueError(\n",
    "                \"`sequence_length` must be an Integer, received `None`.\"\n",
    "            )\n",
    "        if embedding_dim is None:\n",
    "            raise ValueError(\n",
    "                \"`embedding_dim` must be an Integer, received `None`.\"\n",
    "            )\n",
    "        self.vocabulary_size = int(vocabulary_size)\n",
    "        self.sequence_length = int(sequence_length)\n",
    "        self.embedding_dim = int(embedding_dim)\n",
    "        self.embeddings_initializer = keras.initializers.get(\n",
    "            embeddings_initializer\n",
    "        )\n",
    "        self.token_embedding = keras.layers.Embedding(\n",
    "            vocabulary_size,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=clone_initializer(\n",
    "                self.embeddings_initializer\n",
    "            ),\n",
    "            mask_zero=mask_zero,\n",
    "            name=\"token_embedding\"\n",
    "            + str(keras.backend.get_uid(\"token_embedding\")),\n",
    "        )\n",
    "        self.position_embedding = PositionEmbedding(\n",
    "            sequence_length=sequence_length,\n",
    "            initializer=clone_initializer(self.embeddings_initializer),\n",
    "            name=\"position_embedding\"\n",
    "            + str(keras.backend.get_uid(\"position_embedding\")),\n",
    "        )\n",
    "        self.supports_masking = self.token_embedding.supports_masking\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        embedded_tokens = self.token_embedding(inputs)\n",
    "        embedded_positions = self.position_embedding(embedded_tokens)\n",
    "        outputs = embedded_tokens + embedded_positions\n",
    "        return outputs\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return self.token_embedding.compute_mask(inputs, mask=mask)\n",
    "\n",
    "\n",
    "\n",
    "class FNetEncoder(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        intermediate_dim,\n",
    "        dropout=0,\n",
    "        activation=\"relu\",\n",
    "        layer_norm_epsilon=1e-5,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        name=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.dropout = dropout\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.kernel_initializer = keras.initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = keras.initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create layers based on input shape.\n",
    "        feature_size = input_shape[-1]\n",
    "\n",
    "        # Layer Norm layers.\n",
    "        self._mixing_layer_norm = keras.layers.LayerNormalization(\n",
    "            epsilon=self.layer_norm_epsilon\n",
    "        )\n",
    "        self._output_layer_norm = keras.layers.LayerNormalization(\n",
    "            epsilon=self.layer_norm_epsilon\n",
    "        )\n",
    "\n",
    "        # Feedforward layers.\n",
    "        self._intermediate_dense = keras.layers.Dense(\n",
    "            self.intermediate_dim,\n",
    "            activation=self.activation,\n",
    "            kernel_initializer=clone_initializer(self.kernel_initializer),\n",
    "            bias_initializer=clone_initializer(self.bias_initializer),\n",
    "        )\n",
    "        self._output_dense = keras.layers.Dense(\n",
    "            feature_size,\n",
    "            kernel_initializer=clone_initializer(self.kernel_initializer),\n",
    "            bias_initializer=clone_initializer(self.bias_initializer),\n",
    "        )\n",
    "        self._output_dropout = keras.layers.Dropout(rate=self.dropout)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "\n",
    "        def fourier_transform(input):\n",
    "            # Apply FFT on the input and take the real part.\n",
    "            # Before we apply fourier transform, let's convert the dtype of the\n",
    "            # input tensor to complex64.\n",
    "            input = tf.cast(input, tf.complex64)\n",
    "            mixing_output = tf.math.real(tf.signal.fft2d(input))\n",
    "            return mixing_output\n",
    "\n",
    "        def add_and_norm(input1, input2, norm_layer):\n",
    "            return norm_layer(input1 + input2)\n",
    "\n",
    "        def feed_forward(input):\n",
    "            x = self._intermediate_dense(input)\n",
    "            x = self._output_dense(x)\n",
    "            return self._output_dropout(x)\n",
    "\n",
    "        mixing_output = fourier_transform(inputs)\n",
    "\n",
    "        mixing_output = add_and_norm(\n",
    "            inputs, mixing_output, self._mixing_layer_norm\n",
    "        )\n",
    "\n",
    "        feed_forward_output = feed_forward(mixing_output)\n",
    "\n",
    "        x = add_and_norm(\n",
    "            mixing_output, feed_forward_output, self._output_layer_norm\n",
    "        )\n",
    "        return x\n",
    "\n",
    "\n",
    "def fnet_classifier(config: FNetConfig):\n",
    "    input_ids = keras.Input(shape=(None,), dtype=\"int64\", name=\"input_ids\")\n",
    "    x = TokenAndPositionEmbedding(\n",
    "        vocabulary_size=config.vocab_size,\n",
    "        sequence_length=config.max_sequence_length,\n",
    "        embedding_dim=config.embedding_dim,\n",
    "        mask_zero=True,\n",
    "    )(input_ids)\n",
    "\n",
    "    for _ in range(config.num_encoder_blocks):\n",
    "        x = FNetEncoder(intermediate_dim=config.intermediate_dim)(inputs=x)\n",
    "\n",
    "    x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = keras.layers.Dropout(0.1)(x)\n",
    "    outputs = keras.layers.Dense(config.num_labels, activation=\"softmax\")(x)\n",
    "    model = keras.Model(input_ids, outputs, name=\"fnet_classifier\")\n",
    "    return model\n",
    "\n",
    "\n",
    "model = fnet_classifier(FNetConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T19:34:33.489353Z",
     "start_time": "2023-03-30T19:22:04.735483Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-30 19:22:06.002191: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-03-30 19:22:06.098437: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 158s 5s/step - loss: 0.9110 - accuracy: 0.5189\n",
      "Epoch 2/5\n",
      "32/32 [==============================] - 148s 5s/step - loss: 0.6943 - accuracy: 0.5506\n",
      "Epoch 3/5\n",
      "32/32 [==============================] - 147s 5s/step - loss: 0.6376 - accuracy: 0.6471\n",
      "Epoch 4/5\n",
      "32/32 [==============================] - 147s 5s/step - loss: 0.3301 - accuracy: 0.8730\n",
      "Epoch 5/5\n",
      "32/32 [==============================] - 147s 5s/step - loss: 0.2103 - accuracy: 0.9204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e2f42550>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_xla = is_xla_compatible_model(model)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4, jit_compile=use_xla),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    jit_compile=use_xla,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T21:16:25.146697Z",
     "start_time": "2023-03-30T21:16:25.095791Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e1faee55f3ced5625a3cb76335de31f85cad4aa5905c1a154bea41f61f60e02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
