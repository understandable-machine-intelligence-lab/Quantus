{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/motzkus/work/xai-quantification-toolbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xai_quantification_toolbox.measures.model_parameter_randomization_test import *\n",
    "from xai_quantification_toolbox.helpers.similarity_func import *\n",
    "\n",
    "#!pip install captum\n",
    "#!pip install opencv-python\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "from xml.etree import ElementTree\n",
    "import xmltodict\n",
    "import cv2\n",
    "\n",
    "from captum.attr import Saliency, IntegratedGradients\n",
    "#from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Retrieve source code.\n",
    "#from drive.MyDrive.Projects.xai_quantification_toolbox import * #import xaiquantificationtoolbox\n",
    "\n",
    "# Notebook settings.\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load model, data and attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained vgg16 model.\n",
    "model = torchvision.models.vgg16(pretrained=False)\n",
    "model.classifier[-1] = torch.nn.Linear(4096, 20)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"../../xai_discriminability/models/pytorch/vgg16_voc/model.pt\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC2012Sample:\n",
    "    \"\"\" Implements a pascal voc 2012 sample. \"\"\"\n",
    "\n",
    "    def __init__(self, datum, filename, label, one_hot_label, binary_mask):\n",
    "        self.datum = datum\n",
    "        self.filename = filename\n",
    "        self.label = label\n",
    "        self.one_hot_label = one_hot_label\n",
    "        self.binary_mask = binary_mask\n",
    "\n",
    "class VOC2012Dataset:\n",
    "    \"\"\" Implements the pascal voc 2012 dataset. \"\"\"\n",
    "\n",
    "    def __init__(self, datapath, partition, classidx=None):\n",
    "        \"\"\" Initialize pascal voc 2012 dataset. \"\"\"\n",
    "        #super().__init__(datapath, partition)\n",
    "        self.datapath = datapath\n",
    "        self.partition = partition\n",
    "        self.samples = []\n",
    "\n",
    "        self.cmap = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "                     'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\n",
    "                     'tvmonitor']\n",
    "        if not classidx:\n",
    "            self.classes = self.cmap\n",
    "\n",
    "        else:\n",
    "            self.classes = []\n",
    "            for idx in classidx:\n",
    "                self.classes.append(self.cmap[int(idx)])\n",
    "\n",
    "        self.labels = []\n",
    "\n",
    "        if not classidx:\n",
    "            f = open(datapath + \"ImageSets/Main/\" + partition + \".txt\", \"r\")\n",
    "        else:\n",
    "            f = []\n",
    "            for idx in classidx:\n",
    "                with open(datapath + \"ImageSets/Main/\" + self.cmap[int(idx)] + \"_\" + partition + \".txt\", \"r\") as classfile:\n",
    "                    for line in classfile:\n",
    "                        filename, in_class = [value for value in line.split(\" \") if value]\n",
    "                        if in_class.startswith(\"1\") and (filename not in f):\n",
    "                            f.append(filename)\n",
    "\n",
    "        for line in f:\n",
    "            if line.endswith(\"\\n\"):\n",
    "                line = line[:-1]\n",
    "            # get image filepath\n",
    "            self.samples.append(datapath + \"JPEGImages/\" + line + \".jpg\")\n",
    "\n",
    "            # parse annotations\n",
    "            tree = ElementTree.parse(datapath + \"Annotations/\" + line + \".xml\")\n",
    "            xml_data = tree.getroot()\n",
    "            xmlstr = ElementTree.tostring(xml_data, encoding=\"utf-8\", method=\"xml\")\n",
    "            annotation = dict(xmltodict.parse(xmlstr))['annotation']\n",
    "\n",
    "            objects = annotation[\"object\"]\n",
    "\n",
    "            if type(objects) != list:\n",
    "                self.labels.append([objects['name']])\n",
    "\n",
    "            else:\n",
    "                label = []\n",
    "                for object in annotation['object']:\n",
    "                    if type(object) == collections.OrderedDict:\n",
    "                        if object['name'] not in label:\n",
    "                            label.append(object['name'])\n",
    "\n",
    "                self.labels.append(label)\n",
    "        \n",
    "        print(\"{} samples loaded\".format(len(self.samples)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get the datapoint at index. \"\"\"\n",
    "\n",
    "        filename = self.samples[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        image = self.preprocess_image(filename)\n",
    "        one_hot_label = self.preprocess_label(label)\n",
    "        binary_mask = self.preprocess_binary_mask(filename)\n",
    "\n",
    "        sample = VOC2012Sample(\n",
    "            image,\n",
    "            filename,\n",
    "            label,\n",
    "            one_hot_label,\n",
    "            binary_mask\n",
    "        )\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def classname_to_idx(self, class_name):\n",
    "        \"\"\" convert a classname to an index. \"\"\"\n",
    "        return self.cmap.index(class_name)\n",
    "\n",
    "    def preprocess_image(self, image):\n",
    "\n",
    "        read_image = cv2.imread(image, cv2.IMREAD_COLOR)\n",
    "        image_resized = cv2.resize(read_image, (224, 224), interpolation=cv2.INTER_CUBIC)\n",
    "        image_normalized = image_resized.astype(np.float32) / 127.5 - 1.0\n",
    "\n",
    "        return image_normalized\n",
    "\n",
    "    def preprocess_label(self, label):\n",
    "        \"\"\" Convert label to one hot encoding. \"\"\"\n",
    "        one_hot_label = np.zeros(len(self.cmap))\n",
    "\n",
    "        for classname in label:\n",
    "            one_hot_label[self.cmap.index(classname)] = 1\n",
    "\n",
    "        return one_hot_label\n",
    "\n",
    "    def preprocess_binary_mask(self, filename):\n",
    "        \"\"\" Get the bounding box as binary mask.\"\"\"\n",
    "\n",
    "        binary_mask = {}\n",
    "        #filename = extract_filename(filename)\n",
    "        filename = filename.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "        # parse annotations\n",
    "        tree = ElementTree.parse(self.datapath + \"Annotations/\" + filename + \".xml\")\n",
    "        xml_data = tree.getroot()\n",
    "        xmlstr = ElementTree.tostring(xml_data, encoding=\"utf-8\", method=\"xml\")\n",
    "        annotation = dict(xmltodict.parse(xmlstr))['annotation']\n",
    "\n",
    "        width = int(annotation[\"size\"][\"width\"])\n",
    "        height = int(annotation[\"size\"][\"height\"])\n",
    "\n",
    "        # iterate objects\n",
    "        objects = annotation[\"object\"]\n",
    "\n",
    "        if type(objects) != list:\n",
    "            # self.labels.append([objects['name']])\n",
    "            mask = np.zeros((width, height), dtype=int)\n",
    "\n",
    "            mask[int(objects['bndbox']['xmin']):int(objects['bndbox']['xmax']), int(objects['bndbox']['ymin']):int(objects['bndbox']['ymax'])] = 1\n",
    "\n",
    "            binary_mask[objects['name']] = mask\n",
    "\n",
    "        else:\n",
    "            for object in annotation['object']:\n",
    "                if type(object) == collections.OrderedDict:\n",
    "                    if object['name'] in binary_mask.keys():\n",
    "                        mask = binary_mask[object['name']]\n",
    "                    else:\n",
    "                        mask = np.zeros((width, height), dtype=np.uint8)\n",
    "\n",
    "                    mask[int(object['bndbox']['xmin']):int(object['bndbox']['xmax']), int(object['bndbox']['ymin']):int(object['bndbox']['ymax'])] = 1\n",
    "\n",
    "                    binary_mask[object['name']] = mask\n",
    "\n",
    "        # preprocess binary masks to fit shape of image data\n",
    "        for key in binary_mask.keys():\n",
    "            # binary_mask[key] = tf.image.resize(binary_mask[key][:, :, np.newaxis], [224, 224]).numpy().astype(int)\n",
    "            binary_mask[key] = cv2.resize(binary_mask[key], (224, 224), interpolation=cv2.INTER_NEAREST).astype(np.int)[:, :, np.newaxis]\n",
    "\n",
    "        return binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341 samples loaded\n"
     ]
    }
   ],
   "source": [
    "classidx = 4\n",
    "\n",
    "dataset = VOC2012Dataset(\"../../data/VOC2012/\", \"val\", classidx=[classidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [dataset[i] for i in range(10)]\n",
    "#x_batch, y_batch, a_batch, s_batch\n",
    "#a_batch = explain(model, x_batch.to(device), y_batch.to(device), explanation_func=\"Saliency\")\n",
    "x_batch = np.array([sample.datum for sample in data])\n",
    "y_batch = np.array([sample.one_hot_label for sample in data])\n",
    "s_batch = np.array([sample.binary_mask[dataset.cmap[classidx]] for sample in data])[:, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_batch_saliency = explain(model.to(device), x_batch, classidx, explanation_func=\"Saliency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 224, 224])\n",
      "(10, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "print(a_batch_saliency.shape)\n",
    "a_batch_saliency = a_batch_saliency.cpu().numpy()\n",
    "\n",
    "print(s_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Option 1. Evaluate the localization authenticity of attributions in one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.27995628 0.26797861 0.23783904 0.25698847 0.25181538 0.20448238\n",
      " 0.25741893 0.30892056 0.30643702 0.2521463 ]\n"
     ]
    }
   ],
   "source": [
    "# One-liner to measure the random logit results of provided attributions.\n",
    "scores = RandomLogitTest(**{\"similarity_func\": cosine})(model=model, x_batch=x_batch, y_batch=classidx, a_batch=a_batch_saliency, s_batch=s_batch, device=device, **{\"explanation_func\": \"Saliency\"})\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'features.0': 0.03180631744379522, 'features.2': 0.03847731748860453, 'features.5': 0.018548948169646844, 'features.7': 0.03235279232532143, 'features.10': 0.05661198560675393, 'features.12': 0.023859605887504215, 'features.14': -0.017531795066205492, 'features.17': 0.0038134832752349593, 'features.19': -0.0287275706358448, 'features.21': -0.08472879368193446, 'features.24': -0.122168280404858, 'features.26': -0.062006307876476645, 'features.28': -0.09306745284073842, 'classifier.0': -0.08365028616651618, 'classifier.3': -0.09683355277365195, 'classifier.6': -0.08311366499239142}, {'features.0': 0.0893330714653337, 'features.2': 0.09781425446879467, 'features.5': 0.1483903810578024, 'features.7': 0.07082386190186744, 'features.10': 0.05486205564101505, 'features.12': -0.0016602960085096495, 'features.14': 0.06467995152139175, 'features.17': 0.0656785692589272, 'features.19': -0.018550975031434366, 'features.21': -0.05309385641852809, 'features.24': -0.0828503108177044, 'features.26': -0.10986646143182573, 'features.28': -0.08855494504018002, 'classifier.0': -0.09762461819333505, 'classifier.3': -0.08533974929480805, 'classifier.6': -0.086795569619949}, {'features.0': 0.06345714258903232, 'features.2': 0.005542009147994438, 'features.5': 0.03963876436211496, 'features.7': -0.029632564658337204, 'features.10': 0.007942793180596333, 'features.12': 0.03192011674511603, 'features.14': -0.056647426534401885, 'features.17': -0.020891856046562903, 'features.19': 0.03465744348605738, 'features.21': -0.07595644438418137, 'features.24': -0.13266212053822038, 'features.26': -0.07051226501007621, 'features.28': -0.09969392827636579, 'classifier.0': -0.09503530204594762, 'classifier.3': -0.09930873829950879, 'classifier.6': -0.09058019036890594}, {'features.0': 0.16233936250303288, 'features.2': 0.018417286471912313, 'features.5': 0.09484717181122564, 'features.7': 0.004033190474432073, 'features.10': -0.003934428351244544, 'features.12': 0.02990665592656018, 'features.14': -0.05369193754509004, 'features.17': -0.055605673015963246, 'features.19': -0.05476668241800362, 'features.21': -0.09679051357678643, 'features.24': -0.11377279631006332, 'features.26': -0.07718387087512647, 'features.28': -0.11012151608129887, 'classifier.0': -0.11849069560627883, 'classifier.3': -0.11173023307478532, 'classifier.6': -0.11496278791950806}, {'features.0': 0.06738379411019246, 'features.2': 0.052830794399766214, 'features.5': 0.04852392700041026, 'features.7': -0.052322956569050796, 'features.10': -0.004208741078772971, 'features.12': 0.041606303907994725, 'features.14': -0.09349493569588145, 'features.17': -0.008827977754509855, 'features.19': -0.08020080490965183, 'features.21': -0.07291944993804239, 'features.24': -0.08643428327637473, 'features.26': -0.05947599459226597, 'features.28': -0.07846840026907972, 'classifier.0': -0.07646372706422841, 'classifier.3': -0.07268482888439975, 'classifier.6': -0.08737072887834553}, {'features.0': 0.12165373186974164, 'features.2': 0.046643021850201395, 'features.5': -0.004681431994084663, 'features.7': -0.01260126985691064, 'features.10': -0.023337440437830297, 'features.12': 0.007546547331001485, 'features.14': 0.02407699951213389, 'features.17': 0.025855507838941034, 'features.19': -0.02511488096819478, 'features.21': -0.14404025687851968, 'features.24': -0.20184068917050943, 'features.26': -0.1900300600453342, 'features.28': -0.17938345274635178, 'classifier.0': -0.1760514197316562, 'classifier.3': -0.1782595792872064, 'classifier.6': -0.18648148087478866}, {'features.0': 0.056998597908142594, 'features.2': 0.0377375538252469, 'features.5': 0.08113397020795915, 'features.7': 0.038307065987301346, 'features.10': 0.013583650024420421, 'features.12': 0.0621467408732416, 'features.14': 0.013014825046952656, 'features.17': -0.009253245150720043, 'features.19': -0.023509670213935338, 'features.21': -0.022267564193628922, 'features.24': -0.036082455390579714, 'features.26': -0.024758699509393797, 'features.28': -0.03915450891844646, 'classifier.0': -0.03419162656692535, 'classifier.3': -0.04627900678798491, 'classifier.6': -0.04653650628161648}, {'features.0': 0.14100907286727224, 'features.2': 0.060134192067303076, 'features.5': 0.10203140375098536, 'features.7': 0.07241029699824521, 'features.10': 0.07839627201361753, 'features.12': 0.09690232935034597, 'features.14': -0.016702415750267558, 'features.17': 0.02122159076328163, 'features.19': -0.1199492825955976, 'features.21': -0.05006528753767256, 'features.24': -0.05136628511496969, 'features.26': -0.07566847665696078, 'features.28': -0.08658964656301163, 'classifier.0': -0.06623934010869038, 'classifier.3': -0.07613337503742736, 'classifier.6': -0.08360240321768744}, {'features.0': 0.05582414618867565, 'features.2': 0.008523325951212862, 'features.5': 0.04986053990177026, 'features.7': 0.02619448101347211, 'features.10': -0.013535156588797963, 'features.12': -0.011522567837253965, 'features.14': -0.055833348691281225, 'features.17': -0.04478304520447946, 'features.19': -0.09791525412924051, 'features.21': -0.041252323523395425, 'features.24': -0.029492351054934477, 'features.26': -0.03412396287023496, 'features.28': -0.04188743420944209, 'classifier.0': -0.05000068498351205, 'classifier.3': -0.045852313402750595, 'classifier.6': -0.04171871187225998}, {'features.0': 0.08871115003154335, 'features.2': -0.030270030789928626, 'features.5': 0.10623307864602607, 'features.7': 0.02754475202175474, 'features.10': 0.018877430944851666, 'features.12': 0.04703633499090061, 'features.14': -0.01128970259126643, 'features.17': -0.048818901024250606, 'features.19': -0.07407317944430379, 'features.21': -0.045460707956818835, 'features.24': -0.0406630534772772, 'features.26': -0.05444642083712014, 'features.28': -0.057858509950292375, 'classifier.0': -0.06726323794883847, 'classifier.3': -0.06425539515559037, 'classifier.6': -0.07110572150134571}]\n"
     ]
    }
   ],
   "source": [
    "# One-liner to measure the model parameter randomization results of provided attributions.\n",
    "scores = ModelParameterRandomizationTest()(model=model, x_batch=x_batch, y_batch=classidx, a_batch=a_batch_saliency, s_batch=s_batch, device=device, **{\"explanation_func\": \"Saliency\"})\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "separability",
   "language": "python",
   "name": "separability"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
