{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"name":"Tutorial-evaluating-explanations-during-model-training.ipynb","provenance":[{"file_id":"1em6hsNlRKEM6-4W5055GUhMGEsiDn32X","timestamp":1627550362887},{"file_id":"1nMiFlKaXP_F5rbeFwc35BL4SwO2ktnEs","timestamp":1627483591348},{"file_id":"14H0YjeULfWNdvzhVJbiScA0sUcLhGmY0","timestamp":1627472772689},{"file_id":"1tlXprNNO0PAcdH4Wh5ThFWgknpSQTkmd","timestamp":1626956510892}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"bukochgPFg7s"},"source":["## Tutorial - Evaluate explanations during model training\n","\n","This tutorial demonstrates how one can use the library to evaluate how explanations changes while a model is training. We use a pre-trained AlexNet model and Tiny Imagenet dataset to showcase the library's functionality.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEhtGsfLp7dO","executionInfo":{"status":"ok","timestamp":1627560316130,"user_tz":-120,"elapsed":603,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"c893b73d-40df-477b-d6fc-986f110784ac"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MuA8v9jLp-pf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627560323796,"user_tz":-120,"elapsed":7048,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"435f5cd8-24d6-4e92-c961-a246e9d7f0c8"},"source":["!pip install captum\n","!pip install opencv-python\n","!pip install torch==1.8.0 torchvision==0.9.0\t\n","#pip install torch==1.9.0+cu102 torchvision==0.9.1\n","\n","import torch\n","import torchvision\n","from torchvision import transforms\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from captum.attr import *\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","import warnings\n","\n","# Retrieve source code.\n","from drive.MyDrive.Projects.xai_quantification_toolbox import * #import xaiquantificationtoolbox\n","\n","# Notebook settings.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n","%load_ext autoreload\n","%autoreload 2\n","\n","import gc"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: captum in /usr/local/lib/python3.7/dist-packages (0.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from captum) (1.19.5)\n","Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from captum) (1.8.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from captum) (3.2.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->captum) (3.7.4.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (0.10.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (1.3.1)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->captum) (1.15.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.19.5)\n","Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (1.8.0)\n","Requirement already satisfied: torchvision==0.9.0 in /usr/local/lib/python3.7/dist-packages (0.9.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0) (1.19.5)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.0) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mB2QuiaDlu7w"},"source":["### Load 5 classes of Imagenet dataset."]},{"cell_type":"code","metadata":{"id":"PZ6VyL7x26Ue","executionInfo":{"status":"ok","timestamp":1627560411599,"user_tz":-120,"elapsed":1325,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}}},"source":["# TODO. Update to tiny imagenet dataset!\n","\n","# Load datasets and make loaders.\n","test_set = torchvision.datasets.ImageFolder(root='/content/drive/My Drive/imagenet_images',\n","                                            transform=transforms.Compose([transforms.Resize(256),\n","                                                                          transforms.CenterCrop((224, 224)),\n","                                                                          transforms.ToTensor(),\n","                                                                          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n","test_loader = torch.utils.data.DataLoader(test_set, shuffle=True, batch_size=56)\n","\n","# Load a batch of inputs and outputs to use for evaluation.\n","x_batch, y_batch = iter(test_loader).next()\n","x_batch, y_batch = x_batch.to(device), y_batch.to(device)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKwN9uWs29sn"},"source":["### During model training/ fine-tuning calculate max-sensitivity scores of Integrated Gradients explanations."]},{"cell_type":"code","metadata":{"id":"cpv0QMA3uTMK","executionInfo":{"status":"ok","timestamp":1627562801366,"user_tz":-120,"elapsed":413,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}}},"source":["def evaluate_model(model, images, labels, device):\n","    \"\"\"Evaluate torch model given images and lables and return predictions and targets.\"\"\"\n","    model.eval()\n","    logits = torch.Tensor().to(device)\n","    targets = torch.LongTensor().to(device)    \n","    return torch.nn.functional.softmax(torch.cat([logits, model(images)]), dim=1), torch.cat([targets, labels])"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"lbfAkSEtmGym","colab":{"base_uri":"https://localhost:8080/"},"outputId":"06c57a44-3cf5-4c77-bc01-c78c8902538c"},"source":["# Load AlexNet model (only constructor, not with weights).\n","model = torchvision.models.mobilenet_v3_small()\n","\n","# Set necessary configs/ parameters.\n","model.to(device)  \n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","path_model_weights = \"drive/MyDrive/Projects/xai_quantification_toolbox/nbs/resources/imagenet5\"\n","epochs = 5\n","nr_samples = 56\n","max_batches = 12\n","sensitivities = {}\n"," \n","for epoch in range(epochs):\n","    model.train() \n","    \n","    for b, (images, labels) in enumerate(test_loader):\n","        \n","        if b >= max_batches:\n","            break\n","\n","        images, labels = images.to(device), labels.to(device)\n","        logits = model(images)\n","\n","        loss = criterion(logits, labels)\n","        model.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Evaluate model!\n","    predictions, labels = evaluate_model(model, x_batch.to(device), y_batch.to(device), device)\n","    test_acc = np.mean(np.argmax(predictions.cpu().detach().numpy(), axis=1) == labels.detach().cpu().numpy())\n","    \n","    # Explain model (on a few test samples) and measure sensitivies.\n","    sensitivities[epoch] = MaxSensitivity()(model=model, \n","                                            x_batch=x_batch[:nr_samples].cpu().numpy(), \n","                                            y_batch=y_batch[:nr_samples].cpu().numpy(), \n","                                            a_batch=None, \n","                                            **{\"explanation_func\": \"Saliency\", \n","                                                \"device\": device,\n","                                                \"img_size\": 224})\n","    \n","    print(f\"Epoch {epoch+1}/{epochs} - train accuracy: {(100 * test_acc):.2f}% - max sensitivity {np.mean(sensitivities[epoch]):.2f}\")\n","\n","# Save model.\n","torch.save(model.state_dict(), path_model_weights)\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5 - train accuracy: 30.36% - max sensitivity 0.03\n","Epoch 2/5 - train accuracy: 30.36% - max sensitivity 0.05\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7jjJFvJm_BsB","executionInfo":{"status":"ok","timestamp":1627557470179,"user_tz":-120,"elapsed":581,"user":{"displayName":"Anna Hedström","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhbfsluHeZ1mzN6Bsf-1zU62lYHcz183jYjeS63=s64","userId":"05540180366077551505"}},"outputId":"6044f1d4-4c15-4926-e9f3-d4d45a6ba1c2"},"source":["[np.mean(v) for k, v in sensitivities]\n","\n","# Summarise in a dataframe.      \n","df = pd.DataFrame(sensitivities)\n","df[\"avg\"] = df.mean(axis=0)\n","df"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.03605495,\n"," 0.038111627,\n"," 0.035774343,\n"," 0.036996625,\n"," 0.03895698,\n"," 0.035916544,\n"," 0.025491755,\n"," 0.0071537965,\n"," 0.022229657,\n"," 0.0200718,\n"," 0.04412011,\n"," 0.032924026,\n"," 0.043629825,\n"," 0.021562316,\n"," 0.01864284,\n"," 0.027031252,\n"," 0.020951644,\n"," 0.0147696175,\n"," 0.027309624,\n"," 0.034381274,\n"," 0.037629068,\n"," 0.017584994,\n"," 0.025542881,\n"," 0.018253563]"]},"metadata":{"tags":[]},"execution_count":36}]}]}